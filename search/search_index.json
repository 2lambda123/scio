{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/index.html","text":"Ecclesiastical Latin IPA: /ˈʃi.o/, [ˈʃiː.o], [ˈʃi.i̯o] Verb: I can, know, understand, have knowledge.","title":"Scio"},{"location":"/index.html#scio","text":"Scio is a Scala API for Apache Beam and Google Cloud Dataflow inspired by Apache Spark and Scalding.\nScio 0.3.0 and future versions depend on Apache Beam (org.apache.beam) while earlier versions depend on Google Cloud Dataflow SDK (com.google.cloud.dataflow). See this page for a list of breaking changes.\nGetting Started is the best place to start with Scio. If you are new to Apache Beam and distributed data processing, check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts. If you have experience with other Scala data processing libraries, check out this comparison between Scio, Scalding and Spark. Finally check out this document about the relationship between Scio, Beam and Dataflow.\nExample Scio pipelines and tests can be found under scio-examples. A lot of them are direct ports from Beam’s Java examples. See this page for some of them with side-by-side explanation. Also see Big Data Rosetta Code for common data processing code snippets in Scio, Scalding and Spark.\nSee Scio Scaladocs for current API documentation.\nSee scio-contrib for community-supported add-ons for Scio.","title":"Scio"},{"location":"/index.html#getting-help","text":"","title":"Getting help"},{"location":"/index.html#documentation","text":"Getting Started - current API documentation Scio REPL - tutorial for the interactive Scio REPL Scio, Beam and Dataflow - how Scio concepts map to Beam and Dataflow Scio, Scalding and Spark - comparison of these frameworks Runners - how Scio handles Beam runners and runner specific logic Scio data guideline - guideline for common problems Apache Beam - notes on Apache Beam compatibility Changelog - breaking changes in each release FAQ - frequently asked questions Powered By - see who is using Scio in production","title":"Documentation"},{"location":"/index.html#io","text":"Type safe BigQuery - tutorial for the type safe BigQuery API HDFS - using Scio with HDFS files Bigtable - using Scio with Bigtable Avro - using Scio with Avro files Protobuf - using Scio with Protobuf Parquet - using Scio with Parquet files","title":"IO"},{"location":"/index.html#extras","text":"Algebird","title":"Extras"},{"location":"/index.html#internals","text":"ScioIO - new IO system to simplify implementation and stubbing in JobTest OverrideTypeProvider - custom mappings for type-safe BigQuery Kryo - Kryo data serialization Coders - new Magnolia based Coders derivation","title":"Internals"},{"location":"/index.html#further-readings","text":"Big Data Processing at Spotify: The Road to Scio (Part 1) Big Data Processing at Spotify: The Road to Scio (Part 2) The world beyond batch: Streaming 101 The world beyond batch: Streaming 102 Dataflow/Beam & Spark: A Programming Model Comparison VLDB paper on the Dataflow Model","title":"Further Readings"},{"location":"/index.html#presentations","text":"Scio - Big Data on Google Cloud with Scala and Scio - Apache Beam Summit London 2018 Talk Sorry - How Bieber broke Google Cloud at Spotify (slides) - Scala Up North 2017 Talk Scio - Moving to Google Cloud A Spotify Story (slides) - Philly ETE 2017 Talk Scio - A Scala API for Google Cloud Dataflow & Apache Beam (slides) - Scala by the Bay 2016 Talk From stream to recommendation with Cloud Pub/Sub and Cloud Dataflow - GCP NEXT 16 Talk Apache Beam Presentation Materials","title":"Presentations"},{"location":"/index.html#projects-using-or-related-to-scio","text":"scio-contrib - Community-supported add-ons for Scio Featran - A Scala feature transformation library for data science and machine learning Big Data Rosetta Code - Code snippets for solving common big data problems in various platforms. Inspired by Rosetta Code Ratatool - A tool for random data sampling and generation, which includes BigDiffy, a Scio library for pairwise field-level statistical diff of data sets (slides) scio-deep-dive - Building Scio from scratch step by step for an internal training session scala-flow - A lightweight Scala wrapper for Google Cloud Dataflow from Zendesk clj-headlights - Clojure API for Apache Beam, also from Zendesk datasplash - A Clojure API for Google Cloud Dataflow","title":"Projects using or related to Scio"},{"location":"/Getting-Started.html","text":"","title":"Getting Started"},{"location":"/Getting-Started.html#getting-started","text":"First install the Google Cloud SDK and create a Google Cloud Storage bucket for your project, e.g. gs://my-bucket. Make sure it’s in the same region as the BigQuery datasets you want to access and where you want Dataflow to launch workers on GCE.\nScio may need Google Cloud’s application default credentials for features like BigQuery. Run the following command to set it up.\ngcloud auth application-default login","title":"Getting Started"},{"location":"/Getting-Started.html#building-scio","text":"Scio is built using SBT. To build Scio and publish artifacts locally, run:\ngit clone git@github.com:spotify/scio.git\ncd scio\n# 'sbt +publishLocal' to cross build for all Scala versions\n# 'sbt ++$SCALA_VERSION publishLocal' to build for a specific Scala version\nsbt publishLocal\nYou can also specify sbt heap size with -mem, e.g. sbt -mem 8192. Use the SBT_OPTS environment variable for more fine grained settings.\nexport SBT_OPTS=\"-Xmx8G -Xms8G -Xss1M -XX:MaxMetaspaceSize=1G -XX:+CMSClassUnloadingEnabled -XX:ReservedCodeCacheSize=128m\"\nTo ensure the project loads and builds successfully, run the following sbt command so that all custom tasks are executed\nsbt compile test:compile it:compile","title":"Building Scio"},{"location":"/Getting-Started.html#running-the-examples","text":"You can execute the examples locally from SBT. By default pipelines will be executed using the DirectRunner and local filesystem will be used for input and output. Take a look at the examples to find out more.\nneville@localhost scio $ sbt\n[info] ...\n> project scio-examples\n[info] ...\n> runMain com.spotify.scio.examples.WordCount --input=<FILE PATTERN> --output=<DIRECTORY>\nNote Unlike Hadoop, Scio or Dataflow input should be file patterns and not directories, i.e. gs://bucket/path/part-*.txt and not gs://bucket/path. Output on the other hand should be directories just like Hadoop, so gs://bucket/path will produce files like gs://bucket/path/part-00000-of-00005.txt.\nUse the DataflowRunner to execute pipelines on Google Cloud Dataflow service using managed resources in the Google Cloud Platform.\nneville@localhost scio $ sbt -Dbigquery.project=<BILLING_PROJECT>\n[info] ...\n> project scio-examples\n[info] ...\n> set beamRunners := \"DataflowRunner\"\n[info] ...\n> runMain com.spotify.scio.examples.WordCount\n--project=<PROJECT ID>\n--zone=<GCE AVAILABILITY ZONE> --runner=DataflowRunner\n--input=<FILE PATTERN> --output=<DIRECTORY>\nThe Cloud Platform project refers to its name (not number). GCE availability zone should be in the same region as the BigQuery datasets and GCS bucket.\nBy default only DirectRunner is in the library dependencies list. Use set beamRunners := \"<runners>\" to specify additional runner dependencies as a comma separated list, i.e. “DataflowRunner,FlinkRunner”.","title":"Running the Examples"},{"location":"/Getting-Started.html#sbt-project-setup","text":"To create a new SBT project using Giter8 scio-template, simply:\nsbt new spotify/scio-template.g8\nOr add the following to your build.sbt. Replace the direct and Dataflow runner with ones you wish to use. The compiler plugin dependency is only needed for the type safe BigQuery API.\nlibraryDependencies ++= Seq(\n  \"com.spotify\" %% \"scio-core\" % \"0.6.1\",\n  \"com.spotify\" %% \"scio-test\" % \"0.6.1\" % \"test\",\n  \"org.apache.beam\" % \"beam-runners-direct-java\" % \"2.6.0\",\n  \"org.apache.beam\" % \"beam-runners-google-cloud-dataflow-java\" % \"2.6.0\"\n)\n\naddCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.1\" cross CrossVersion.full)","title":"SBT project setup"},{"location":"/Getting-Started.html#bigquery-settings","text":"You may need a few extra settings to use BigQuery queries as pipeline input.\nsbt -Dbigquery.project=<PROJECT-ID>\nbigquery.project: GCP project to make BigQuery requests with at compile time. bigquery.secret: By default the credential in Google Cloud SDK will be used. A JSON secret file can be used instead with -Dbigquery.secret=secret.json.","title":"BigQuery Settings"},{"location":"/Getting-Started.html#options","text":"The following options should be specified when running a job on Google Cloud Dataflow service.\n--project - The project ID for your Google Cloud Project. This is required if you want to run your pipeline using the Cloud Dataflow managed service. --zone - The Compute Engine availability zone for launching worker instances to run your pipeline.\nFor pipeline execution parameters and optimization, see the following documents.\nSpecifying Pipeline Execution Parameters Service Optimization and Execution\nThe defaults should work well for most cases but we sometimes tune the following parameters manually. - --workerMachineType - start with smaller types like n1-standard-1 and go up if you run into memory problem. n1-standard-4 works well for a lot of our memory hungry jobs. - --maxNumWorkers - avoid setting it to too high, i.e. 1000 or close to quota, since that reduces available instances for other jobs and more workers means more expensive shuffle. - --diskSizeGb - increase this if you run into disk space problem during shuffle, or alternatively optimize code by replacing groupByKey with reduceByKey or sumByKey. - --workerDiskType - specify SSD for jobs with really expensive shuffles. See a list of disk types here. Also see this page about persistent disk size and type. - --network - specify this if you use VPN to communicate with external services, e.g. HDFS on an on-premise cluster.\nMore Dataflow pipeline specific options available can be found in DataflowPipelineOptions and super interfaces. Some more useful ones are from DataflowPipelineWorkerPoolOptions.\nDataflowWorkerHarnessOptions#getWorkerCacheMb affects side input performance but needs an extra step to enable. See this FAQ item.\nThere are a few more experimental settings that might help specific scenarios: - --experiments=shuffle_mode=service - use external shuffle service instead of local disk - --experiments=enable_custom_bigquery_sink - new custom sink that works around certain limitations when writing to BigQuery - --experiments=worker_region-<REGION> - use specified Google Cloud region instead of zone for more flexible capacity","title":"Options"},{"location":"/examples.html","text":"","title":""},{"location":"/io/index.html","text":"","title":"IO"},{"location":"/io/index.html#io","text":"","title":"IO"},{"location":"/io/Type-Safe-BigQuery.html","text":"","title":"BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#bigquery","text":"","title":"BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#background","text":"NOTE that there are currently two BigQuery dialects, the legacy query syntax and the new SQL 2011 standard. The SQL standard is highly recommended since it generates dry-run schemas consistent with actual result and eliminates a lot of edge cases when working with records in a type-safe manner. To use standard SQL, prefix your query with #standardsql.","title":"Background"},{"location":"/io/Type-Safe-BigQuery.html#tablerow","text":"BigQuery rows are represented as TableRow in the BigQuery Java API which is basically a Map<String, Object>. Fields are accessed by name strings and values must be cast or converted to the desired type, both of which are error prone process.","title":"TableRow"},{"location":"/io/Type-Safe-BigQuery.html#type-safe-bigquery","text":"The type safe BigQuery API in Scio represents rows as case classes and generates TableSchema converters automatically at compile time with the following mapping logic:\nNullable fields are mapped to Option[T]s Repeated fields are mapped to List[T]s Records are mapped to nested case classes Timestamps are mapped to Joda Time Instant\nSee documentation for BigQueryType for the complete list of supported types.","title":"Type safe BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#type-annotations","text":"There are 4 annotations for type safe code generation.","title":"Type annotations"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-fromtable","text":"This expands a class with fields that map to a BigQuery table. Note that class Row has no body definition and is expanded by the annotation at compile time based on actual table schema.\n@BigQueryType.fromTable(\"publicdata:samples.gsod\")\nclass Row","title":"BigQueryType.fromTable"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-fromquery","text":"This expands a class with output fields from a SELECT query. A dry run is executed at compile time to get output schema and does not incur additional cost.\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [publicdata:samples.gsod]\")\nclass Row\nThe query string may also contain \"%s\"s and additional arguments for parameterized query. This could be handy for log type data.\n// generate schema at compile time from a specific date\n@BigQueryType.fromQuery(\"SELECT user, url FROM [my-project:logs.log_%s]\", \"20160101\")\nclass Row\n\n// generate new query strings at runtime\nval newQuery = Row.query.format(args(0))\nThere’s also a $LATEST placeholder for table partitions. The latest common partition for all tables with the placeholder will be used.\n// generate schema at compile time from the latest date available in both my-project:log1.log_* and my-project:log2.log_*\n@BigQueryType.fromQuery(\n  \"SELECT user, url, action FROM [my-project:log1.log_%s] JOIN [my-project:log2.log_%s] USING user\",\n  \"$LATEST\", \"$LATEST\")\nclass Row\n\n// generate new query strings at runtime\nval newQuery = Row.query.format(args(0), args(0))","title":"BigQueryType.fromQuery"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-fromschema","text":"This annotation gets schema from a string parameter and is useful in tests.\n@BigQueryType.fromSchema(\n  \"\"\"\n    |{\n    |  \"fields\": [\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f1\", \"type\": \"INTEGER\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f2\", \"type\": \"FLOAT\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f3\", \"type\": \"BOOLEAN\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f4\", \"type\": \"STRING\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f5\", \"type\": \"TIMESTAMP\"}\n    |    ]\n    |}\n  \"\"\".stripMargin)\nclass Row","title":"BigQueryType.fromSchema"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-totable","text":"This annotation works the other way around. Instead of generating class definition from a BigQuery schema, it generates BigQuery schema from a case class definition.\n@BigQueryType.toTable\ncase class Result(user: String, url: String, time: Long)\nFields in the case class and the class itself can also be annotated with @description which propagates to BigQuery schema.\n@BigQueryType.toTable\n@description(\"A list of users mapped to the urls they visited\")\ncase class Result(user: String, \n                  url: String,\n                  @description(\"Milliseconds since Unix epoch\") time: Long)","title":"BigQueryType.toTable"},{"location":"/io/Type-Safe-BigQuery.html#annotation-parameters","text":"Note that due to the nature of Scala macros, only string literals and multi-line strings with optional .stripMargin are allowed as parameters to BigQueryType.fromTable, BigQueryType.fromQuery and BigQueryType.fromSchema.\nThese are OK:\n@BigQueryType.fromTable(\"project-id:dataset-id.table-id\")\nclass Row1\n\n@BigQueryType.fromQuery(\n  \"\"\"\n    |SELECT user, url\n    |FROM [project-id:dataset-id.table-id]\n  \"\"\".stripMargin)\nclass Row2\nAnd these are not:\n@BigQueryType.fromQuery(\"SELECT \" + args(1) + \" FROM [\" + args(2) + \"]\")\nclass Row1\n\nval sql = \"SELECT \" + args(1) + \" FROM [\" + args(2) + \"]\"\n@BigQueryType.fromQuery(sql)\nclass Row2","title":"Annotation parameters"},{"location":"/io/Type-Safe-BigQuery.html#companion-objects","text":"Classes annotated with the type safe BigQuery API have a few more convenience methods.\nschema: TableSchema - BigQuery schema fromTableRow: (TableRow => T) - TableRow to case class converter toTableRow: (T => TableRow) - case class to TableRow converter toPrettyString(indent: Int = 0) - pretty string representation of the schema\nscio> @BigQueryType.fromTable(\"publicdata:samples.gsod\")\n     | class Row\ndefined class Row\ndefined object Row\n\nscio> println(Row.toPrettyString(2))\n(\n  station_number: Int,\n  wban_number: Int,\n  year: Int,\n  month: Int,\n  day: Int,\n  mean_temp: Double,\n  num_mean_temp_samples: Int,\n  mean_dew_point: Double,\n  num_mean_dew_point_samples: Int,\n  mean_sealevel_pressure: Double,\n  num_mean_sealevel_pressure_samples: Int,\n  mean_station_pressure: Double,\n  num_mean_station_pressure_samples: Int,\n  mean_visibility: Double,\n  num_mean_visibility_samples: Int,\n  mean_wind_speed: Double,\n  num_mean_wind_speed_samples: Int,\n  max_sustained_wind_speed: Double,\n  max_gust_wind_speed: Double,\n  max_temperature: Double,\n  max_temperature_explicit: Boolean,\n  min_temperature: Double,\n  min_temperature_explicit: Boolean,\n  total_precipitation: Double,\n  snow_depth: Double,\n  fog: Boolean,\n  rain: Boolean,\n  snow: Boolean,\n  hail: Boolean,\n  thunder: Boolean,\n  tornado: Boolean)\nIn addition, BigQueryType.fromTable and BigQueryTable.fromQuery generate table: String and query: String respectively that refers to parameters in the original annotation.\nUser defined companion objects may interfere with macro code generation so for now do not provide one to a case class annotated with @BigQueryType.toTable, i.e. object Row.","title":"Companion objects"},{"location":"/io/Type-Safe-BigQuery.html#using-type-safe-bigquery","text":"","title":"Using type safe BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#type-safe-bigquery-with-scio","text":"To enable type safe BigQuery for ScioContext:\nimport com.spotify.scio.bigquery._\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [publicdata:samples.gsod]\")\nclass Row\n\n@BigQueryType.toTable\ncase class Result(month: Long, tornado_count: Long)\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  sc.typedBigQuery[Row]()  // query string from Row.query\n    .flatMap(r => if (r.tornado.getOrElse(false)) Seq(r.month) else Nil)\n    .countByValue\n    .map(kv => Result(kv._1, kv._2))\n    .saveAsTypedBigQuery(args(\"output\"))  // schema from Row.schema\n  sc.close()\n}","title":"Type safe BigQuery with Scio"},{"location":"/io/Type-Safe-BigQuery.html#type-safe-bigqueryclient","text":"Annotated classes can be used with the BigQueryClient directly too.\nimport com.spotify.scio.bigquery.client.BigQuery\n\nval bq = new BigQuery()\nval rows = bq.getTypedRows[Row]()\nbq.writeTypedRows(\"project-id:dataset-id.table-id\", rows.toList)","title":"Type safe BigQueryClient"},{"location":"/io/Type-Safe-BigQuery.html#using-type-safe-bigquery-directly-with-beams-io-library","text":"If there are any BigQuery I/O operations supported in the Apache Beam client but not exposed in Scio, you may choose to use the Beam transform directly using Scio’s .saveAsCustomOutput() option:\nimport org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO\n\nval bigQueryType = BigQueryType[Foo]\nval tableRows: SCollection[Foo] = ...\n\ntableRows\n  .map(bigQueryType.toTableRow)\n  .saveAsCustomOutput(\n    \"custom bigquery IO\",\n    BigQueryIO\n      .writeTableRows()\n      .to(\"my-project:my-dataset.my-table\")\n      .withSchema(bigQueryType.schema)\n      .withCreateDisposition(...)\n      .withWriteDisposition(...)\n      .withFailedInsertRetryPolicy(...)\n    )","title":"Using type safe BigQuery directly with Beam’s IO library"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-and-intellij-idea","text":"See the FAQ for making IntelliJ happy with type safe BigQuery.","title":"BigQueryType and IntelliJ IDEA"},{"location":"/io/Type-Safe-BigQuery.html#custom-types-and-validation","text":"See OverrideTypeProvider for details about the custom types and validation mechanism.","title":"Custom types and validation"},{"location":"/io/HDFS.html","text":"","title":"HDFS"},{"location":"/io/HDFS.html#hdfs","text":"","title":"HDFS"},{"location":"/io/HDFS.html#settings-for-scio-version-0-4-0","text":"Hadoop configuration is loaded from ‘core-site.xml’ and ‘hdfs-site.xml’ files based upon the HADOOP_CONF_DIR and YARN_CONF_DIR environment variables. Set one of them to the location of your Hadoop configuration directory.","title":"Settings for Scio version >= 0.4.0"},{"location":"/io/HDFS.html#settings-for-scio-version-0-4-0","text":"To access HDFS from a Scio job, Hadoop configuration files (core-site.xml, hdfs-site.xml, etc.) must be available in classpath of both runner’s client and workers. There is multiple ways to achieve it:\nplace your configuration files in java startup classpath via java -cp <classpath> place your configuration files in src/main/resources\nThe following example handles local, GCS and HDFS in the same job code.\nimport com.spotify.scio.hdfs._\n\nval input = args(\"input\")\nval output = args(\"output\")\n\nval pipe = if (input.startsWith(\"hdfs://\")) {\n  sc.hdfsTextFile(input)  // HDFS\n} else {\n  sc.textFile(input)  // local or GCS\n}\n\nval result = pipe\n  .map(/*...*/)\n  .reduce(/*...*/)\n\nif (output.startsWith(\"hdfs://\")) {\n  result.saveAsHdfsTextFile(output)  // HDFS\n} else {\n  result.saveAsTextFile(output)  // local or GCS\n}","title":"Settings for Scio version < 0.4.0"},{"location":"/io/HDFS.html#running-locally","text":"You local environment needs access to the name and data nodes in the Hadoop cluster when running a job locally.","title":"Running locally"},{"location":"/io/HDFS.html#running-on-dataflow-service","text":"When running a job on the Dataflow managed service, worker VM instances need access to the Hadoop cluster. Set --network/--subnetwork to one that has proper setup, e.g. VPN to your on-premise cluster or another cloud provider.","title":"Running on Dataflow service"},{"location":"/io/HDFS.html#use-hdfs-from-scio-repl","text":"Assuming that you have your configuration files in hadoop-conf.jar, for example:\n$ jar -tf hadoop-conf.jar\nMETA-INF/\nMETA-INF/MANIFEST.MF\ncore-site.xml\nhdfs-site.xml\nAssembly HDFS submodule:\n$ sbt 'project scio-hdfs' assembly\nStart Scio REPL:\n$ java -cp scio-repl/target/scala-2.11/scio-repl-0.3.4-SNAPSHOT.jar:scio-hdfs/target/scala-2.11/scio-hdfs-assembly-0.3.4-SNAPSHOT.jar:hadoop-conf.jar com.spotify.scio.repl.ScioShell","title":"Use HDFS from Scio REPL"},{"location":"/io/HDFS.html#common-issues","text":"","title":"Common issues"},{"location":"/io/HDFS.html#permission-denied-user-root-access-write-inode-","text":"By default your Dataflow job is run as `root` user on worker containers. If you are using Simple Authentication for HDFS, Scio provides a parameter in HDFS sink methods to specify remote user via `username` parameter.\n\n#### java.io.IOException: No FileSystem for scheme: hdfs\n\nThis is a common issue with fat jars built with [sbt-assembly](https://github.com/sbt/sbt-assembly) plugin. It's related to Hadoop filesystem registration services file - there are multiple jars that provide the same file with configuration, in your case most probably only one is picked (and it's not the one holding configuration for HDFS filesystem).\n\n###### Possible solutions\n\n* 1st solution - if you are using [sbt-assembly](https://github.com/sbt/sbt-assembly) plugin - merge those service files, so in your sbt merge configuration add strategy to filter distinct lines, like this:\nmergeStrategy in assembly <<= (mergeStrategy in assembly) { (old) => { case PathList(“META-INF”, “services”, “org.apache.hadoop.fs.FileSystem”) => MergeStrategy.filterDistinctLines case s => old(s) } } ```\n2nd solution - give sbt-pack a try, it creates a directory/tarball of all dependency jars without explicit merging/fatjars.\nMore on this/related issues here.","title":"Permission denied: user=root, access=WRITE, inode=…"},{"location":"/io/Bigtable.html","text":"","title":"Bigtable"},{"location":"/io/Bigtable.html#bigtable","text":"First please read Google’s official doc.","title":"Bigtable"},{"location":"/io/Bigtable.html#bigtable-example","text":"This depends on APIs from scio-bigtable and imports from com.spotify.scio.bigtable._.\nLook at example here.","title":"Bigtable example"},{"location":"/io/Bigtable.html#common-issues","text":"","title":"Common issues"},{"location":"/io/Bigtable.html#size-of-the-cluster-vs-dataflow-cluster","text":"As a general note when writing to Bigtable from Dataflow you should at most use the # of Bigtable nodes you have * 3 cpus. Otherwise Bigtable will be overwhelmed and throttle the writes (and the reads)","title":"Size of the cluster vs Dataflow cluster"},{"location":"/io/Bigtable.html#cell-compression","text":"Bigtable doesn’t compress cell values > 1Mb","title":"Cell compression"},{"location":"/io/Bigtable.html#jetty-alpn-npn-has-not-been-properly-configured","text":"Check that your versions of grpc-netty, netty-handler, and netty-tcnative-boringssl-static are compatible.","title":"Jetty ALPN/NPN has not been properly configured"},{"location":"/io/Bigtable.html#bigtableio","text":"The BigtableIO included in the Dataflow SDK is not recommended for use. It is not written by the Bigtable team and is significantly less performant than the HBase Bigtable Dataflow connector. Please see the example above for the recommended API.","title":"BigtableIO"},{"location":"/io/Bigtable.html#key-structure","text":"Your row key should not contain common parts at the beginning of the key, doing so would overload specific Bigtable nodes. For example, if your row is identifiable by user-id and date key - do NOT use date,user-id, instead use user-id,date or even better in case of date use Bigtable version/timestamp. Read more about row key design over here.","title":"Key structure"},{"location":"/io/Bigtable.html#performance","text":"Read Google doc.","title":"Performance"},{"location":"/io/Bigtable.html#bigtable-vs-datastore","text":"If you require replacement for Cassandra, Bigtable is probable the most straightforward replacement in GCP. Bigtable white paper. To quote the paper - think of Bigtable as:\na sparse, distributed, persistent multidimensional sorted map. The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.\nBigtable is replicated only within a single zone. Bigtable does not support transactions, that said all operations are atomic at the row level.\nThink of Datastore as distributed, persistent, fully managed key-value store, with support for transactions. Datastore is replicated across multiple datacenters thus making it theoretically more available than Bigtable (as of today).\nRead more about Bigtable here, and more about Datastore over here.","title":"Bigtable vs Datastore"},{"location":"/io/Avro.html","text":"","title":"Avro"},{"location":"/io/Avro.html#avro","text":"","title":"Avro"},{"location":"/io/Avro.html#read-avro-files","text":"Scio comes with support for reading Avro files. Avro supports generic or specific records, Scio supports both via the same method (avroFile), but depending on the type parameter.","title":"Read Avro files"},{"location":"/io/Avro.html#specific-record-","text":"# SpecificRecordClass is compiled from Avro schema files\nsc.avroFile[SpecificRecordClass](\"gs://path-to-data/lake/part-*.avro\")\n  .map(record => ???)\n# `record` is of your SpecificRecordClass type","title":"Specific record:"},{"location":"/io/Avro.html#generic-record-","text":"import org.apache.avro.generic.GenericRecord\nsc.avroFile[GenericRecord](\"gs://path-to-data/lake/part-*.avro\", yourAvroSchema)\n  .map(record => ???)\n# `record` is of GenericRecord type","title":"Generic record:"},{"location":"/io/Avro.html#write-avro-files","text":"Scio comes with support for writing Avro files. Avro supports generic or specific records, Scio supports both via the same method (saveAsAvroFile), but depending on the type of the content of SCollection.","title":"Write Avro files"},{"location":"/io/Avro.html#specific-record-","text":"# type of Avro specific records will hold information about schema,\n# therefor Scio will figure out the schema by itself\nsc.map(<build Avro specific records>)\n  .saveAsAvroFile(\"gs://path-to-data/lake/output\")","title":"Specific record:"},{"location":"/io/Avro.html#generic-record-","text":"# writing Avro generic records requires additional argument `schema`\nsc.map(<build Avro generic records>)\n  .saveAsAvroFile(\"gs://path-to-data/lake/output\", schema = yourAvroSchema)","title":"Generic record:"},{"location":"/io/Avro.html#rules-for-schema-evolution","text":"Unless impossible, provide default values for your fields. New field must have a default value. You can only delete field which has default value. Do not change data type of an existing fields. If needed add a new field to the schema. Do not rename existing fields. If needed use aliases.","title":"Rules for schema evolution"},{"location":"/io/Avro.html#common-issues-guidelines","text":"Follow Avro guidelines, especially the one about schema evolution Wherever possible use specific records Use Builder pattern to construct Avro records","title":"Common issues/guidelines"},{"location":"/io/Protobuf.html","text":"","title":"Protobuf"},{"location":"/io/Protobuf.html#protobuf","text":"","title":"Protobuf"},{"location":"/io/Protobuf.html#read-protobuf-files","text":"Scio comes with custom and efficient support for reading Protobuf files via protobufFile method, for example:\n// FooBarProto is a Protobuf generated class (must be a subclass of Protobuf's `Message`)\nsc.protobufFile[FooBarProto](\"gs://path-to-data/lake/part-*.protobuf.avro\")\n  .map( message => ... )\n// `message` is of type FooBarProto\nImportant: in most cases the input files should have been previously written by Scio. The reason is that Scio assumes that serialized Protobuf message is stored inside bytes Avro record.\nIf you want to read serialized protobuf messages directly from a file, one solution is to use textFile followed by a map to parse your messages.","title":"Read Protobuf files"},{"location":"/io/Protobuf.html#write-protobuf-files","text":"Scio comes with custom and efficient support for writing Protobuf files via saveAsProtobufFile method, for example:\n// FooBarProto is a Protobuf generated class (must be a subclass of Protobuf's `Message`)\nsc.map(<build a protobuf message>)\n  .saveAsProtobufFile(\"gs://path-to-data/lake/protos-out\")","title":"Write Protobuf files"},{"location":"/io/Protobuf.html#file-format","text":"Scio’s Protobuf file is backed by Avro with the following schema:\n{\n  \"type\" : \"record\",\n  \"name\" : \"AvroBytesRecord\",\n  \"fields\" : [ {\n    \"name\" : \"bytes\",\n    \"type\" : \"bytes\"\n  } ]\n}\nAvro gives us a block based file format with compression, split and combine support. Protobuf binary is stored in the bytes field of AvroBytesRecord.\nStarting with Scio 0.2.6, the Protobuf schema also is stored as a JSON string in the Avro file metadata under the key protobuf.generic.schema. You can get the schema or JSON records using the proto-tools command line tool from gcs-tools (available in our homebrew tap). Conversion between Protobuf schema, binary and JSON is done via the protobuf-generic library.\nbrew tap spotify/public\nbrew install gcs-proto-tools\nproto-tools getschema data.protobuf.avro\nproto-tools tojson data.protobuf.avro","title":"File format"},{"location":"/io/Protobuf.html#common-issues","text":"","title":"Common issues"},{"location":"/io/Protobuf.html#scalapb","text":"If you end up using ScalaPB, make sure to use java based message class as input/output type, Scala based message class does not inherit from Protobuf’s Message class. To generate both Scala and Java classes add (to your build.sbt):\nimport com.trueaccord.scalapb.{ScalaPbPlugin => PB}\nPB.javaConversions in PB.protobufConfig := true","title":"ScalaPB"},{"location":"/io/Parquet.html","text":"","title":"Parquet"},{"location":"/io/Parquet.html#parquet","text":"Scio supports reading and writing Parquet files as Avro records. It also includes parquet-avro-extra macros for generating column projections and row predicates using idiomatic Scala syntax. Also see Avro page on reading and writing regular Avro files.","title":"Parquet"},{"location":"/io/Parquet.html#read-avro-parquet-files","text":"When reading Parquet files, only Avro specific records are supported.\nTo read a Parquet file with column projections and row predicates:\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\n\n// Macros for generating column projections and row predicates\nval projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)\nval predicate = Predicate[TestRecord](x => x.getIntField > 0 && x.getBooleanField)\n\nsc.parquetAvroFile[TestRecord](\"input.parquet\", projection, predicate)\n  // Map out projected fields right after reading\n  .map(r => (r.getIntField, r.getStringField, r.getBooleanField))\nNote that the result TestRecords are not complete Avro objects. Only the projected columns (intField, stringField, booleanField) are present while the rest are null. These objects may fail serialization and it’s recommended that you map them out to tuples or case classes right after reading.\nAlso note that predicate logic is only applied when reading actual Parquet files but not in JobTest. To retain the filter behavior while using mock input, it’s recommend that you do the following.\nval projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)\n\n// Build both native filter function and Parquet FilterPredicate\n// case class Predicates[T](native: T => Boolean, parquet: FilterPredicate)\nval predicate = Predicate.build[TestRecord](x => x.getIntField > 0 && x.getBooleanField)\n\nsc.parquetAvroFile[TestRecord](\"input.parquet\", projection, predicate.parquet)\n  // filter natively with the same logic in case of mock input in `JobTest`\n  .filter(predicate.native)","title":"Read Avro Parquet files"},{"location":"/io/Parquet.html#write-avro-parquet-files","text":"Both Avro generic and specific records are supported when writing.\nimport com.spotify.scio.parquet.avro._\n\n# type of Avro specific records will hold information about schema,\n# therefor Scio will figure out the schema by itself\ninput.map(<build Avro specific records>)\n     .saveAsParquetAvroFile(\"gs://path-to-data/lake/output\")\n\n# writing Avro generic records requires additional argument `schema`\ninput.map(<build Avro generic records>)\n     .saveAsParquetAvroFile(\"gs://path-to-data/lake/output\", schema = yourAvroSchema)","title":"Write Avro Parquet files"},{"location":"/Scio-Unit-Tests.html","text":"","title":"Testing"},{"location":"/Scio-Unit-Tests.html#testing","text":"To write Scio unit tests you will need to add the following dependency to your build.sbt\nlibraryDependencies ++= Seq(\n.......\n\"com.spotify\" %% \"scio-test\" % scioVersion % Test,\nTo run the test, you can run the following commands. You can skip the first two lines if you already ran them and are still in the sbt shell.\n$ sbt\n> project scio-examples\n> test\nClick on this link for more Scala testing tasks.","title":"Testing"},{"location":"/Scio-Unit-Tests.html#test-entire-pipeline","text":"We will use the WordCountTest to explain how Scio tests work. WordCount is the pipeline under test. Full example code for WordCountTest and other test examples can be found here.\nLet’s walk through the details of the test: The test class should extend the PipelineSpec which is a trait for unit testing pipelines.\nThe inData variable holds the input data for your test and the expected variable contains the expected results after your pipeline processes the inData. The WordCount pipeline counts the occurrence of each word, the given the input data , we should expect a count of a=3, b=3, c=1 etc\nval inData = Seq(\"a b c d e\", \"a b a b\", \"\")\n val expected = Seq(\"a: 3\", \"b: 3\", \"c: 1\", \"d: 1\", \"e: 1\")\nUsing JobTest, you can test the entire pipeline. Specify the type of the class under test, in this case it is com.spotify.scio.examples.WordCount.type . The args function takes the list of command line arguments passed to the main function of WordCount. The WordCount’s main function expects input and output arguments passed to it.\n\"WordCount\" should \"work\" in {\n JobTest[com.spotify.scio.examples.WordCount.type]\n   .args(\"--input=in.txt\", \"--output=out.txt\")\n   .input(TextIO(\"in.txt\"), inData)\n   .output(TextIO(\"out.txt\"))(_ should containInAnyOrder(expected))\n   .run()\n}\nThe input function injects your input test data. Note that the TestIO[T] should match the input source used in the pipeline e.g. TextIO for sc.textFile, AvroIO for sc.avro. The TextIO id (“in.txt”) should match the one specified in the args.\nThe output function evaluates the output of the pipeline using the provided assertion from the SCollectionMatchers. More info on SCollectionMatchers can be found here. In this example, we are asserting that the output of the pipeline should contain an SCollection with elements that in the expected variable in any order. Also, note that the TestIO[T] should match the output used in the pipeline e.g. TextIO for sc.saveAsTextFile\nThe run function will run the pipeline.","title":"Test entire pipeline"},{"location":"/Scio-Unit-Tests.html#test-for-pipeline-with-sideinput","text":"We will use the SideInputJoinExamples test in JoinExamplesTest to illustrate how to write a test for pipelines with sideinputs. The SideInputJoinExamples pipeline has two input sources, one for eventsInfo and the other for countryInfo. CountryInfo is used as a sideinput to join with eventInfo.\nSince we have two input sources, we have to specify both in the JobTest. Note that the injected data type should match one expected by the sink.\n\"SideInputJoinExamples\" should \"work\" in {\n JobTest[com.spotify.scio.examples.cookbook.SideInputJoinExamples.type]\n   .args(\"--output=out.txt\")\n   .input(BigQueryIO(ExampleData.EVENT_TABLE), eventData)\n   .input(BigQueryIO(ExampleData.COUNTRY_TABLE), countryData)\n   .output(TextIO(\"out.txt\"))(_ should containInAnyOrder(expected))\n   .run()\n}","title":"Test for pipeline with sideinput"},{"location":"/Scio-Unit-Tests.html#test-for-pipeline-with-sideoutput","text":"SideInOutExampleTest shows an example of how to test pipelines with sideoutputs. Each sideoutput is evaluated using the output function. The ids for TextIO e.g. “out1.txt” should match the ones specified in the args.\n\"SideInOutExample\" should \"work\" in {\n JobTest[SideInOutExample.type]\n .args(\"--input=in.txt\",\n     \"--stopWords=stop.txt\",\n     \"--output1=out1.txt\",\n     \"--output2=out2.txt\",\n     \"--output3=out3.txt\",\n     \"--output4=out4.txt\")\n   .output(TextIO(\"out1.txt\"))(_ should containInAnyOrder(Seq.empty[String]))\n   .output(TextIO(\"out2.txt\"))(_ should containInAnyOrder(Seq.empty[String]))\n   ………………...\n   }\n   .run()","title":"Test for pipeline with sideoutput"},{"location":"/Scio-Unit-Tests.html#test-partial-pipeline","text":"To test a section of a pipeline, use runWithContext. The TriggerExample.extractFlowInfo test in TriggerExampleTest tests only the extractFlowInfo part of the pipeline.\nThe data variable hold the test data and sc.parallelize will transform the input iterable to a SCollection of strings. TriggerExample.extractFlowInfo will be executed using the ScioContext and you can then specify assertions against the result of the pipeline.\nval data = Seq(\n \"01/01/2010 00:00:00,1108302,94,E,ML,36,100,29,0.0065,66,9,1,0.001,74.8,1,9,3,0.0028,71,1,9,\"\n   + \"12,0.0099,67.4,1,9,13,0.0121,99.0,1,,,,,0,,,,,0,,,,,0,,,,,0\",\n \"01/01/2010 00:00:00,\"\n   + \"1100333,5,N,FR,9,0,39,,,9,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,\"\n)\n\nrunWithContext { sc =>\n val r = TriggerExample.extractFlowInfo(sc.parallelize(data))\n r should haveSize(1)\n r should containSingleValue((\"94\", 29))\n}","title":"Test partial pipeline"},{"location":"/Scio-Unit-Tests.html#test-for-pipeline-with-windowing","text":"We will use the LeaderBoardTest to explain how to test Windowing in Scio. The full example code is found here. LeaderBoardTest also extends PipelineSpec. The function under test is the LeaderBoard.calculateTeamScores. This function calculates teams scores within a fixed window with the following the window options: * Calculate the scores every time the window ends * Calculate an early/“speculative” result from partial data, 5 minutes after the first element in our window is processed (withEarlyFiring) * Accept late entries (and recalculates based on them) only if they arrive within the allowedLateness duration.\nIn this test, we are testing calculateTeamScores for when all of the elements arrive on time, i.e. before the watermark.\nWe specify, the fixed window size is set to 20 minutes and allowedLateness is 1 hour. private val allowedLateness = Duration.standardHours(1) private val teamWindowDuration = Duration.standardMinutes(20)\nFirst, we have to create an input stream representing an unbounded SCollection of type GameActionInfo using the testStreamOf. Each element is assigned a timestamp representing when each event occurred. In the code snippet above, we start at epoch equal zero, by setting watermark to 0 in the advanceWatermarkTo.\nWe add GameActionInfo elements with varying timestamps, and we advanced the watermark to 3 minutes. At this point, all elements are on time because they came before the watermark advances to 3 minutes.\nval stream = testStreamOf[GameActionInfo]\n// Start at the epoch\n .advanceWatermarkTo(baseTime)\n // add some elements ahead of the watermark\n .addElements(\n   event(blueOne, 3, Duration.standardSeconds(3)),\n   event(blueOne, 2, Duration.standardMinutes(1)),\n   event(redTwo, 3, Duration.standardSeconds(22)),\n   event(blueTwo, 5, Duration.standardSeconds(3))\n )\n // The watermark advances slightly, but not past the end of the window\n .advanceWatermarkTo(baseTime.plus(Duration.standardMinutes(3)))\nWe then more GameActionInfo elements and advance the watermark to infinity by calling the advanceWatermarkToInfinity. Similarly, these elements are also on time because the watermark is infinity.\n.addElements(event(redOne, 1, Duration.standardMinutes(4)),\n            event(blueOne, 2, Duration.standardSeconds(270)))\n// The window should close and emit an ON_TIME pane\n.advanceWatermarkToInfinity\nTo run the test, we use the runWithContext, this will run calculateTeamScores using the ScioContext. In calculateTeamScores, we pass the SCollection we created above using testStreamOf. The IntervalWindow specifies the window for which we want to assert the SCollection of elements created by calculateTeamScores. We want to assert that elements with initial window of 0 to 20 minutes were on time. Next we assert, using inOnTimePane that the SCollection elements are equal to the expected sums.\nrunWithContext { sc =>\n val teamScores =\n   LeaderBoard.calculateTeamScores(sc.testStream(stream), teamWindowDuration, allowedLateness)\n\n val window = new IntervalWindow(baseTime, teamWindowDuration)\n teamScores should inOnTimePane(window) {\n   containInAnyOrder(Seq((blueOne.team, 12), (redOne.team, 4)))\n }\n}\nScio provides more SCollection assertions such as inWindow, inCombinedNonLatePanes, inFinalPane, and inOnlyPane. You can find the full list here. More information on testing unbounded pipelines can be found here.","title":"Test for pipeline with windowing"},{"location":"/Scio-REPL.html","text":"","title":"REPL"},{"location":"/Scio-REPL.html#repl","text":"The Scio REPL is an extension of the Scala REPL, with added functionality that allows you to interactively experiment with Scio. Think of it as a playground to try out things.","title":"REPL"},{"location":"/Scio-REPL.html#quick-start","text":"You can either install Scio REPL via our Homebrew tap on a Mac or download the pre-built jar on other platforms.","title":"Quick start"},{"location":"/Scio-REPL.html#homebrew","text":"brew tap spotify/public\nbrew install scio\nscio-repl","title":"Homebrew"},{"location":"/Scio-REPL.html#pre-built-jar","text":"To download pre-built jar of Scio REPL, find version you are interested in on the release page, and download the REPL jar from Downloads section.\n$ java -jar scio-repl-<version>.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio>\nA ScioContext is created on REPL startup as sc and a starting point to most operations. Use tab completion, history and other REPL goodies to play around.","title":"Pre-built jar"},{"location":"/Scio-REPL.html#start-from-sbt-console-scala-2-11-x-only-","text":"$ git clone git@github.com:spotify/scio.git\nCloning into 'scio'...\nremote: Counting objects: 9336, done.\nremote: Compressing objects: 100% (275/275), done.\nremote: Total 9336 (delta 139), reused 0 (delta 0), pack-reused 8830\nReceiving objects: 100% (9336/9336), 1.76 MiB | 0 bytes/s, done.\nResolving deltas: 100% (3509/3509), done.\nChecking connectivity... done.\n$ cd scio\n$ sbt scio-repl/run","title":"Start from SBT console (Scala 2.11.x+ only)"},{"location":"/Scio-REPL.html#build-repl-jar-manually","text":"You can also build REPL jar from source.\n$ git clone git@github.com:spotify/scio.git\nCloning into 'scio'...\nremote: Counting objects: 9336, done.\nremote: Compressing objects: 100% (275/275), done.\nremote: Total 9336 (delta 139), reused 0 (delta 0), pack-reused 8830\nReceiving objects: 100% (9336/9336), 1.76 MiB | 0 bytes/s, done.\nResolving deltas: 100% (3509/3509), done.\nChecking connectivity... done.\n$ cd scio\n$ sbt scio-repl/assembly","title":"Build REPL jar manually"},{"location":"/Scio-REPL.html#sbt-project-from-scio-template","text":"Projects generated from scio-template.g8 have built-in REPL. Run sbt repl/run from the project root.","title":"sbt project from scio-template"},{"location":"/Scio-REPL.html#tutorial","text":"","title":"Tutorial"},{"location":"/Scio-REPL.html#local-pipeline","text":"Let’s start with simple local-mode word count example:\nscio> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty)).countByValue.map(_.toString).saveAsTextFile(\"/tmp/local_wordcount\")\nscio> sc.close()\nscio> wordCount.waitForResult().value.take(3).foreach(println)\n(but,4)\n(via,4)\n(Hadoop,6)\nMake sure README.md is in the current directory. This example counts words in local file using a local runner (DirectRunner and writes result in a local file. The pipeline and actual computation starts on sc.close(). The last command take 3 lines from results and prints them.","title":"Local pipeline"},{"location":"/Scio-REPL.html#local-pipeline-","text":"In the next example we will spice things up a bit and read data from GCS:\nscio> :newScio\nscio> val shakespeare = sc.textFile(\"gs://dataflow-samples/shakespeare/hamlet.txt\")\nscio> val wordCount = shakespeare.flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty)).countByValue.map(_.toString).saveAsTextFile(\"/tmp/gcs-wordcount\")\nscio> sc.close()\nscio> wordCount.waitForResult().value.take(3).foreach(println)\n(frown'st,1)\n(comfortable,13)\n(diversity,1)\nEach Scio context is associated with one and only one pipeline. The previous instance of sc was used for the local pipeline example and cannot be reused anymore. The first magic command, :newScio creates a new context as sc. The pipeline still performs computation locally, but reads data from Google Cloud Storage (it could also be BigQuery, Datastore, etc). This example may take a bit longer due to additional network overhead.","title":"Local pipeline ++"},{"location":"/Scio-REPL.html#dataflow-service-pipeline","text":"To create a Scio context for Google Cloud Dataflow service, add Dataflow pipeline options when starting the REPL. The same options will also be used by :newScio when creating new context. For example:\n$ java -jar scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<stagin-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio> val shakespeare = sc.textFile(\"gs://dataflow-samples/shakespeare/*\")\nscio> val wordCount = shakespeare.flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty)).countByValue.map(_.toString).saveAsTextFile(\"gs://<gcs-output-dir>\")\nscio> sc.close()\nscio> wordCount.waitForResult().value.take(3).foreach(println)\n(decreased,1)\n('shall',2)\n(War,4)\nIn this case we are reading data from GCS and performing computation in GCE virtual machines managed by Dataflow service. The last line is an example of reading data from GCS files to local memory after a context is closed. Most write operations in Scio return Future[Tap[T]] where a Tap[T] encapsulates some dataset that can be re-opened in another context or directly.\nUse :scioOpts to view or update Dataflow options inside the REPL. New options will be applied the next time you create a context.","title":"Dataflow service pipeline"},{"location":"/Scio-REPL.html#ad-hoc-local-mode","text":"You may start the REPL in distributed mode and run pipelines to aggregate from large datasets, and play around the results in local mode. You can create a local Scio context any time with :newLocalScio <name> and use it for local computations.\nscio> :newLocalScio lsc\nLocal Scio context available as 'lsc'","title":"Ad-hoc local mode"},{"location":"/Scio-REPL.html#bigquery-example","text":"In this example we will read some data from BigQuery and process it in Dataflow. We shall count number of tornadoes per month from a public sample dataset. Scio will do its best to find your configured Google Cloud project, but you can also specify it explicitly via -Dbigquery.project option.\n$ java -jar -Dbigquery.project=<project-id> scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<stagin-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio> val tornadoes = sc.bigQuerySelect(\"SELECT tornado, month FROM [clouddataflow-readonly:samples.weather_stations]\")\nscio> val counts = tornadoes.flatMap(r => if (r.getBoolean(\"tornado\")) Seq(r.getLong(\"month\")) else Nil).countByValue.map(kv => TableRow(\"month\" -> kv._1, \"tornado_count\" -> kv._2))\nscio> val result = counts.take(3).materialize\nscio> sc.close()\nscio> result.waitForResult().value.foreach(println)\n{month=4, tornado_count=5}\n{month=3, tornado_count=6}\n{month=5, tornado_count=6}\nIn this example we combine power of BigQuery and flexibility of Dataflow. We first query BigQuery table, perform a couple of transformations and take (take(3)) some data back locally (materialize) to view the results.","title":"BigQuery example"},{"location":"/Scio-REPL.html#bigquery-project-id","text":"Scio REPL will do its best to find your configured Google Cloud project, without the need to explicitly specifying bigquery.project property. It will search for project-id in this specific order:\nbigquery.project java system property GCLOUD_PROJECT java system property GCLOUD_PROJECT environmental variable gcloud config files: scio named configuration default configuration\nThis means that you can always set bigquery.project and it will take precedence over other configurations. Read more about gcloud configuration here.","title":"BigQuery project id"},{"location":"/Scio-REPL.html#i-o-commands","text":"There are few built-in commands for simple file I/O.\n// Read from an Avro, text, CSV or TSV file on local filesystem or GCS.\ndef readAvro[T : ClassTag](path: String): Iterator[T]\ndef readText(path: String): Iterator[String]\ndef readCsv[T: RowDecoder](path: String,\n                           sep: Char = ',',\n                           header: Boolean = false): Iterator[T]\ndef readTsv[T: RowDecoder](path: String\n                           sep: Char = '\\t',\n                           header: Boolean = false): Iterator[T]\n\n// Write to an Avro, text, CSV or TSV file on local filesystem or GCS.\ndef writeAvro[T: ClassTag](path: String, data: Seq[T]): Unit\ndef writeText(path: String, data: Seq[String]): Unit\ndef writeCsv[T: RowEncoder](path: String, data: Seq[T],\n                            sep: Char = ',',\n                            header: Seq[String] = Seq.empty): Unit\ndef writeTsv[T: RowEncoder](path: String, data: Seq[T],\n                            sep: Char = '\\t',\n                            header: Seq[String] = Seq.empty): Unit","title":"I/O Commands"},{"location":"/Scio-REPL.html#tips","text":"","title":"Tips"},{"location":"/Scio-REPL.html#multi-line-code","text":"While in the REPL, use :paste magic command to paste or write multi-line code\nscio> :paste\n// Entering paste mode (ctrl-D to finish)\n\ndef evenNumber(x: Int) = x % 2 == 0\nval evenNumbers = sc.parallelize(1 to 100).filter(evenNumber)\n\n// Exiting paste mode, now interpreting.\n\nevenNumber: (x: Int)Boolean\nevenNumbers: com.spotify.scio.values.SCollection[Int] = com.spotify.scio.values.SCollectionImpl@14fe085b\n\nscio> evenNumbers.saveAsTextFile(\"/tmp/even\")\nscio> sc.close()","title":"Multi-line code"},{"location":"/Scio-REPL.html#running-jobs-asynchronously","text":"When using REPL and Dataflow service consider using the non-blocking DataflowRunner for a more interactive experience. To start:\njava -jar scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<stagin-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio> sc.parallelize(1 to 100).map( _.toString ).saveAsTextFile(\"gs://<output>\")\nres0: scala.concurrent.Future[com.spotify.scio.io.Tap[String]] = scala.concurrent.impl.Promise$DefaultPromise@1399ad68\nscio> val result = sc.close()\n[main] INFO org.apache.beam.runners.dataflow.DataflowRunner - Executing pipeline on the Dataflow Service, which will have billing implications related to Google Compute Engine usage and other Google Cloud Services.\n[main] INFO org.apache.beam.runners.dataflow.util.PackageUtil - Uploading 3 files from PipelineOptions.filesToStage to staging location to prepare for execution.\n[main] INFO org.apache.beam.runners.dataflow.util.PackageUtil - Uploading PipelineOptions.filesToStage complete: 2 files newly uploaded, 1 files cached\nDataflow SDK version: 2.9.0\nscio> result.state\nres1: org.apache.beam.sdk.PipelineResult.State = RUNNING\nNote that now sc.close() doesn’t block and wait until job completes and gives back control of the REPL right away. Use ScioResult and Future[Tap[T]]s to check for progress, results and orchestrate jobs.","title":"Running jobs asynchronously"},{"location":"/Scio-REPL.html#multiple-scio-contexts","text":"You can use multiple Scio context objects to work with several pipelines at the same time, simply use magic :newScio <context name>, for example:\nscio> :newScio c1\nScio context available as 'c1'\nscio> :newScio c2\nScio context available as 'c2'\nscio> :newLocalScio lc\nScio context available as 'lc'\nYou can use those in combination with DataflowRunner to run multiple pipelines in the same session or wire them with for comprehension over futures.","title":"Multiple Scio contexts"},{"location":"/Scio-REPL.html#bigquery-client","text":"Whenever possible leverage BigQuery! @BigQueryType annotations enable type safe and civilized integration with BigQuery inside Scio. Here is example of using the annotations and BigQuery client to read and write typed data directly without Scio context.\n$ java -jar -Dbigquery.project=<project-id> scio-repl-0.7.0.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio> @BigQueryType.fromQuery(\"SELECT tornado, month FROM [clouddataflow-readonly:samples.weather_stations]\") class Row\nscio> val tornadoes = bq.getTypedRows[Row]()\nscio> tornadoes.next.month\nres0: Option[Long] = Some(5)\nscio> bq.writeTypedRows(\"project-id:dataset-id.table-id\", tornadoes.take(100).toList)","title":"BigQuery client"},{"location":"/Scio-REPL.html#out-of-memory-exception","text":"In case of OOM exceptions, like for example:\nscio> res1.waitForResult().value.next\nException in thread \"main\"\nException: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"main\"\nsimply increase the size of the heap - be reasonable about the amount of data and heap size though.\nExample of REPL startup with 2GiB of heap size:\n$ java -Xmx2g -jar scio-repl-0.7.0.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\n\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio> Runtime.getRuntime().maxMemory();\nres1: Long = 1908932608","title":"Out of memory exception"},{"location":"/Scio-REPL.html#what-is-the-type-of-an-expression-","text":"Use build in :t magic, :t displays the type of an expression without evaluating it. Example:\nscio> :t sc.textFile(\"README\").flatMap(_.split(\"[^a-zA-Z']+\")).filter(_.nonEmpty).map(_.length)\ncom.spotify.scio.values.SCollection[Int]\nLearn more about magic keywords via scio> :help","title":"What is the type of an expression?"},{"location":"/internals/index.html","text":"","title":"Internals"},{"location":"/internals/index.html#internals","text":"","title":"Internals"},{"location":"/internals/Coders.html","text":"","title":"Coder Typeclass"},{"location":"/internals/Coders.html#coder-typeclass","text":"Starting from Scio 0.7.0,","title":"Coder Typeclass"},{"location":"/internals/Coders.html#coder-in-apache-beam","text":"As per Beam’s documentation\nWhen Beam runners execute your pipeline, they often need to materialize the intermediate data in your PCollections, which requires converting elements to and from byte strings. The Beam SDKs use objects called Coders to describe how the elements of a given PCollection may be encoded and decoded.\nSo anytime you create a SCollection[T], Beam needs to know how to go from an instance of T to an array of bytes, and from that array of bytes to an intance of T.\nThe Beam SDK has a class called Coder that roughly looks like this:\npublic abstract class Coder<T> implements Serializable {\n  public abstract void encode(T value, OutputStream outStream);\n  public abstract T decode(InputStream inStream);\n}\nBeam provides built-in Coders for various basic Java types (Integer, Long, Double, etc.). But anytime you create a new class, and that that class is used in a SCollection, a beam coder needs to be provided.\ncase class Foo(x: Int, s: String)\n\nval sc: SCollection[Foo] = ??? // Beam will need an org.apache.beam.sdk.coders.Coder[Foo]","title":"Coder in Apache Beam"},{"location":"/internals/Coders.html#scio-0-7-0","text":"In Scio 0.6.x and below, Scio would delegate this serialization process to Kryo. Kryo’s job is to automagically “generate” the serialization logic for any type. The benefit is you don’t really have to care about serialization most of the time when writing pipelines with Scio. Using Beam, you would need to explicitly set the coder everytime you use a PTtransform.\nWhile it saves a lot of work, it also has a few drawbacks:\nKryo coders can be really inefficient. Especially if you forget to register your classes using a custom KryoRegistrar. The only way to be sure Kryo coders are correctly registered is to write tests and run them with a specific option: (see kryoRegistrationRequired=true). Kryo coders are very dynamic and it can be hard to know exactly which coder is used for a given class. Kryo coders do not always play well with Beam, and sometime can cause weird runtime exceptions. For example, Beam may sometimes throw an IllegalMutationException because of the default Kryo coder implementation.","title":"Scio < 0.7.0"},{"location":"/internals/Coders.html#scio-0-7-0","text":"In Scio 0.7.0 and above, the Scala compiler will try to find the correct instance of Coder at compile time. In most cases, the compiler should be able to either directly find a proper Coder implementation, or derive one automatically.\nPlease note that Scio wraps Beam coders in its own Coder definition: com.spotify.scio.coders.Coder","title":"Scio >= 0.7.0"},{"location":"/internals/Coders.html#buit-in-coder-instances","text":"Here’s an example REPL session that demonstrate it:\nscala> import com.spotify.scio.coders._\nimport com.spotify.scio.coders._\nscala> Coder[Int] // Try to find a Coder instance for Int\nres0: com.spotify.scio.coders.Coder[Int] = <some implementation> // the compiler found a proper instance\nHere the compiler just found a proper Coder for integers.\nScio also provides Coders for commons collections types:\nscala> Coder[List[String]] // Try to find a Coder instance for List[String]\nres1: com.spotify.scio.coders.Coder[List[String]] = <some other implementation>","title":"Buit-in Coder instances"},{"location":"/internals/Coders.html#automatically-derived-coder-instances","text":"If you define a case class, the compiler can automatically derive a Coder for that class\nscala> case class Demo(i: Int, s: String, xs: List[Double])\ndefined class Demo\nscala> Coder[Demo]\nres2: com.spotify.scio.coders.Coder[Demo] = <yet another instance>\nsealed class hierarchy are also supported:\nscala> :pas\n// Entering paste mode (ctrl-D to finish)\nsealed trait Top\nfinal case class TA(anInt: Int, aString: String) extends Top\nfinal case class TB(anDouble: Double) extends Top\n// Exiting paste mode, now interpreting.\n\ndefined trait Top\ndefined class TA\ndefined class TB\n\nscala> Coder[Top]\nres3: com.spotify.scio.coders.Coder[Top] = <generated instance for Top>","title":"Automatically derived Coder instances"},{"location":"/internals/Coders.html#coder-fallbacks","text":"Sometimes, no Coder instance can be found, and it’s impossible to automatically derive one. In that case, Scio will fallback to a Kryo coder for that specific type, and if the scalac flag -Xmacro-settings:show-coder-fallback=true is set, a warning message will be displayed at compile time. This message should help you fix the warning.\nscala> Coder[java.util.Locale]\n<console>:15:\n Warning: No implicit Coder found for type, using Kryo fallback:\n\n   >> java.util.Locale\n\n\n  Scio will use a fallback Kryo coder instead.\n\n  If a type is not supported, consider implementing your own implicit Coder for this type.\n  It is recommended to declare this Coder in your class companion object:\n\n       object Locale {\n         import com.spotify.scio.coders.Coder\n         import org.apache.beam.sdk.coders.AtomicCoder\n\n         implicit def coderLocale: Coder[Locale] =\n           Coder.beam(new AtomicCoder[Locale] {\n             def decode(in: InputStream): Locale = ???\n             def encode(ts: Locale, out: OutputStream): Unit = ???\n           })\n       }\n\n  If you do want to use a Kryo coder, be explicit about it:\n\n       implicit def coderLocale: Coder[Locale] = Coder.kryo[Locale]\n\nres4: com.spotify.scio.coders.Coder[java.util.Locale] = Fallback(java.util.Locale)\nHere for example, the compiler could not find a proper instance of Coder[Locale], and suggest you implement one yourself.\nNote that this message is not limited to direct invocation of fallback. For example, if you declare a case class that uses Locale internally, the compiler will show the same warning:\nscala> case class Demo2(i: Int, s: String, xs: List[java.util.Locale])\n\ndefined class Demo2\n\nscala> Coder[Demo2]\n<console>:17:\n Warning: No implicit Coder found for type, using Kryo fallback:\n\n   >> java.util.Locale\n\n\n  Scio will use a fallback Kryo coder instead.\n\n  If a type is not supported, consider implementing your own implicit Coder for this type.\n  It is recommended to declare this Coder in your class companion object:\n\n       object Locale {\n         import com.spotify.scio.coders.Coder\n         import org.apache.beam.sdk.coders.AtomicCoder\n\n         implicit def coderLocale: Coder[Locale] =\n           Coder.beam(new AtomicCoder[Locale] {\n             def decode(in: InputStream): Locale = ???\n             def encode(ts: Locale, out: OutputStream): Unit = ???\n           })\n       }\n\n  If you do want to use a Kryo coder, be explicit about it:\n\n       implicit def coderLocale: Coder[Locale] = Coder.kryo[Locale]\n\nres5: com.spotify.scio.coders.Coder[Demo2] = Transform(Record([Lscala.Tuple2;@60736dd9),<function1>)\nScio will still use a “proper” Coder for Int, String and List. Only the serialization of Locale instances is delegated to Kryo.","title":"Coder fallbacks"},{"location":"/internals/Coders.html#upgrating-to-v0-7-0-or-above-migrating-to-static-coder","text":"Migrating to Scio 0.7.x from an older version is likely to break a few things at compile time in your project. See the complete v0.7.0 Migration Guide for more information.","title":"Upgrating to v0.7.0 or above: Migrating to static coder"},{"location":"/internals/Kryo.html","text":"","title":"Kryo"},{"location":"/internals/Kryo.html#kryo","text":"Scio uses a framework called Kryo to serialize objects that need to be shuffled between workers. Network throughput can easily become a bottleneck for your pipeline, so optimizing serialization is an easy win. If you use Dataflow’s shuffler service, you pay per GB shuffled so you can save money even if shuffling is not a bottleneck.\nBy registering your classes at compile time, Kryo can serialize far more efficiently that doing it on the fly. The default serializer includes the full classpath of each class you serialize. By pre-registering the classes you want to serialize, Kryo can replace this with an int as identifier. For some pipelines we observed a 30-40% reduction of bytes shuffled.","title":"Kryo"},{"location":"/internals/Kryo.html#how-to-enable-kryoregistrar","text":"Add the following class. You can rename it, but its name has to end in KryoRegistrar. Also make sure that the Macro Paradise plugin is enabled for your project.\npackage x\n\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.{AllScalaRegistrar, IKryoRegistrar, toRich}\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    // Take care of common Scala classes; tuples, Enumerations, ...\n    val reg = new AllScalaRegistrar\n    reg(k)\n\n    k.registerClasses(List(\n      // All classes that might be shuffled, e.g.:\n      classOf[foo.bar.MyClass],\n\n      // Class that takes type parameters:\n      classOf[java.util.ArrayList[_]],\n      // But you can also explicitly do:\n      classOf[Array[Byte]],\n\n      // Private class; cannot use classOf:\n      Class.forName(\"com.spotify.scio.extra.sparkey.LocalSparkeyUri\"),\n\n      // Some common Scala objects\n      None.getClass,\n      Nil.getClass\n    ))\n  }\n}\nNote: since Dataflow may shuffle data at any point, you not only have to include classes that are explicitly shuffled (through join or groupBy), but also those returned by map, flatMap, etc.","title":"How to enable KryoRegistrar"},{"location":"/internals/Kryo.html#verifying-it-works","text":"You can add the following class to your test folder; it will enforce registration of classes during your tests. It only works if you actually run your job in tests, so be sure to include a JobTest or so for each pipeline you run.\npackage x\n\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.IKryoRegistrar\n\n/** Makes sure we don't forget to register encoders, enabled only in tests not to crash production. */\n@KryoRegistrar\nclass TestKryoRegistrar extends IKryoRegistrar {\n  def apply(k: Kryo): Unit = {\n    k.setRegistrationRequired(true)\n  }\n}\nIf you missed registering any classes, you’ll get an error that looks like this:\n[info]   java.lang.IllegalArgumentException: Class is not registered: org.apache.avro.generic.GenericData$Record\n[info] Note: To register this class use: kryo.register(org.apache.avro.generic.GenericData$Record.class);\nWhich you solve by adding classOf[GenericData.Record] or Class.forName(\"org.apache.avro.generic.GenericData$Record\") in MyKryoRegistrar.","title":"Verifying it works"},{"location":"/internals/OverrideTypeProvider.html","text":"","title":"OverrideTypeProvider"},{"location":"/internals/OverrideTypeProvider.html#overridetypeprovider","text":"The OverrideTypeProvider trait allows the user to provide custom mappings from BigQuery types to custom Scala types.\nThis can be used for a number of use cases: * Using higher level types in Scio in order to be explicit about what your data is * Custom code can be run when you create new objects to do things like data validation or simple transformation\nThe methods in the Scala trait allow you to inspect the incoming types from BigQuery and decide if you’d like to provide an alternative type mapping to your own custom type. You also must tell Scio how to convert your types back into BigQuery data types.","title":"OverrideTypeProvider"},{"location":"/internals/OverrideTypeProvider.html#setup","text":"Once you implement the OverrideTypeProvider with your own custom types you can supply it to the OverrideTypeProviderFinder by specifying a JVM System property as below.\nSystem.setProperty(\n  \"override.type.provider\",\n  \"com.spotify.scio.bigquery.validation.SampleOverrideTypeProvider\")\nSince this feature uses Scala macros you must do this at initialization time. One easy way to do this is in the build.sbt file for your project. This would look like below.\ninitialize in Test ~= { _ => System.setProperty(\n  \"override.type.provider\",\n  \"com.spotify.scio.bigquery.validation.SampleOverrideTypeProvider\")\n}\nCurrently only one OverrideTypeProvider is allowed per sbt project.\nThis provider is loaded using Reflection at macro expansion time and at runtime as well.\nIf this System property isn’t specified then Scio falls back to the normal default behavior.","title":"Setup"},{"location":"/internals/OverrideTypeProvider.html#implementation","text":"Custom implementations of the OverrideTypeProvider should implement the methods as described below.\ndef shouldOverrideType(tfs: TableFieldSchema)\nThis is the first point of entry and is called when we use macros to create case classes for fromQuery, fromSchema, and fromTable.\ndef getScalaType(c: blackbox.Context)(tfs: TableFieldSchema)\nThis is called when the above shouldOverrideType returns true. Expected return value is a c.Tree representing the Scala type you’d like to use for this mapping.\ndef shouldOverrideType(c: blackbox.Context)(tpe: c.Type)\nThis is called when we do conversions to and from a TableRow internally and your generated case class.\ndef createInstance(c: blackbox.Context)(tpe: c.Type, tree: c.Tree)\nThis is called when the above shouldOverrideType returns true. Expected return value is a c.Tree representing how to create a new instance of your custom Scala type.\ndef shouldOverrideType(tpe: Type)\nThis is called at runtime when we do any operations on the schema directly.\ndef getBigQueryType(tpe: Type)\nThis is called when the above shouldOverrideType returns true. It should return the String representation for the BigQuery column type for your class and field now.\ndef initializeToTable(c: blackbox.Context)(modifiers: c.universe.Modifiers,\n                                           variableName: c.universe.TermName,\n                                           tpe: c.universe.Tree)\nThis is called once per field when we extend the case classes for toTable examples.","title":"Implementation"},{"location":"/internals/ScioIO.html","text":"","title":"ScioIO"},{"location":"/internals/ScioIO.html#scioio","text":"Scio 0.7.0 introduces a new ScioIO[T] trait to simplify IO implementation and stubbing in JobTest. This page lists some major changes to this new API.","title":"ScioIO"},{"location":"/internals/ScioIO.html#dependencies","text":"Avro and BigQuery logic was decoupled from scio-core as part of the refactor.\nBefore 0.7.0 scio-core depends on scio-avro and scio-bigquery ScioContext and SCollection[T] include Avro, object, Protobuf and BigQuery IO methods out of the box After 0.7.0 scio-core no longer depends on scio-avro and scio-bigquery Import com.spotify.scio.avro._ to get Avro, object, Protobuf IO methods on ScioContext and SCollection[T] Import com.spotify.scio.bigquery._ to get BigQuery IO methods on ScioContext and SCollection[T]","title":"Dependencies"},{"location":"/internals/ScioIO.html#scioio-t-for-jobtest","text":"As part of the refactor TestIO[T] was replaced by ScioIO[T] for JobTest. Some of them were moved to different packages for consistency but most test code should work with minor import changes. Below is a list of ScioIO[T] implementations.\ncom.spotify.scio.avro AvroIO[T] ObjectFileIO[T] ProtobufIO[T] com.spotify.scio.bigquery BigQueryIO[T] TableRowJsonIO where T =:= TableRow com.spotify.scio.io DatastoreIO where T =:= Entity PubsubIO[T] TextIO where T =:= String CustomIO[T] for use with ScioContext#customInput and SCollection#customOutput com.spotify.scio.bigtable BigtableIO[T] where T =:= Row for input and T =:= Mutation for output This replaces BigtableInput and BigtableOutput com.spotify.scio.cassandra CassandraIO[T] com.spotify.scio.elasticsearch ElasticsearchIO[T] com.spotify.scio.extra.json JsonIO[T] com.spotify.scio.jdbc JdbcIO[T] com.spotify.scio.parquet.avro ParquetAvroIO[T] com.spotify.scio.spanner SpannerIO[T] com.spotify.scio.tensorflow TFRecordIO where T =:= Array[Byte] TFExampleIO where T =:= Example","title":"ScioIO[T] for JobTest"},{"location":"/internals/ScioIO.html#using-scioio-t-directly","text":"2 methods, ScioContext#read and SCollection#write were added to leverage ScioIO[T] directly without needing the extra ScioContext#{textFile,AvroFile,...} and SCollection#saveAs{TextFile,AvroFile,...} syntactic sugar. See WordCountScioIO and WordCountScioIOTest for concrete examples.","title":"Using ScioIO[T] directly"},{"location":"/migrations/index.html","text":"","title":"Migration Guides"},{"location":"/migrations/index.html#migration-guides","text":"","title":"Migration Guides"},{"location":"/migrations/v0.7.0-Migration-Guide.html","text":"","title":"Scio 0.7.0"},{"location":"/migrations/v0.7.0-Migration-Guide.html#scio-0-7-0","text":"Scio 0.7.0 comes with major improvements over previous versions. The overall goal is to make Scio safer, faster, more consistent and easier to extend by leveraging Scala’s type system more and refactoring its internals.\nThe new milestone has been profiled more than ever and will improve the performances of your pipeline. In some cases, the improvement can be very significant.\nScio 0.7.0 also includes, like every release, a number of bugfixes.","title":"Scio 0.7.0"},{"location":"/migrations/v0.7.0-Migration-Guide.html#whats-new-","text":"","title":"What’s new ?"},{"location":"/migrations/v0.7.0-Migration-Guide.html#new-ios-api","text":"In this version, we’ve refactored the implementation of IOs. Scio now provides a new class ScioIO that you can extend to support new types of IOs. ScioContext now has a new method called read and SCollection now has a new method write. Both take an instance of a class extending ScioIO as a parameter and may read from any source, or write to any target.\nAll existing IOs (GCS, BigQuery, BigTable, etc.) have been rewritten to use the new IO API.\nRead more: ScioIO","title":"New IOs api"},{"location":"/migrations/v0.7.0-Migration-Guide.html#new-coders","text":"Scio 0.7.0 also ship with a new Coder implementation that statically resolve the correct Coder for a given type at compile time. In previous versions, Scio would infer the correct coder implementation at runtime, which could lead to poor performances and occasionally, exceptions at runtime.\nRead more: Coders.","title":"New “static” coders"},{"location":"/migrations/v0.7.0-Migration-Guide.html#performances-improvements-benchmarks","text":"Thanks to the new static coders implementation, and because of the time we spend on profiling, Scio 0.7.0 should overall be more efficient than previous versions.","title":"Performances improvements & benchmarks"},{"location":"/migrations/v0.7.0-Migration-Guide.html#automated-migration","text":"Scio 0.7 comes with a set of scalafix rules that will do most of the heavy lifting automatically. Before you go through any manual step, we recommend you start by applying those rules.\nStart by adding the scalafix sbt plugin to your project/plugins.sbt file\naddSbtPlugin(\"ch.epfl.scala\" % \"sbt-scalafix\" % \"0.9.0\")\nlaunch sbt and run scalafixEnable\n> scalafixEnable\n[info] Set current project to my-amazing-scio-pipeline (in build file:/Users/julient/Documents/my-amazing-scio-pipeline/)","title":"Automated migration"},{"location":"/migrations/v0.7.0-Migration-Guide.html#prepare-your-tests","text":"RUN THIS BEFORE UPGRADING SCIO\nYou’ll need to prepare your tests code for migration. For this to run properly, you code needs to compile.\nRun the following command in the sbt shell:\n> test:scalafix github:spotify/scio/FixAvroIO\n[info] Running scalafix on 78 Scala sources\n[success] Total time: 7 s, completed Oct 17, 2018 12:49:31 PM\nOnce FixAvroIO has been applied, you can go ahead and upgrade Scio to 0.7.x in your build file. After you have set Scio’s version in your build.sbt, make sure to either restart or reload sbt.\nYou can now run the automated migration rules. At the moment, we support 4 rules:\nName Description AddMissingImports Add the required imports to access sources / sinks on ScioContext and SCollection RewriteSysProp Replace sys.call(...) by the new syntax FixAvroIO Fix uses of AvroIO in tests BQClientRefactoring Automatically migrate from BigQueryClient to the new BigQuery client\nYou can see all the rules here.\nIn your sbt shell, you can now apply the 3 other rules:\n> scalafix github:spotify/scio/AddMissingImports\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 16 s, completed Oct 17, 2018 12:01:31 PM\n\n> scalafix github:spotify/scio/RewriteSysProp\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 6 s, completed Oct 17, 2018 12:34:00 PM\n\n> scalafix github:spotify/scio/BQClientRefactoring\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 3 s, completed Oct 17, 2018 12:34:20 PM\nAt that point you can try to compile your code and fix the few compilation errors left. The next sections of this guide should contain all the information you need to fix everything.","title":"Prepare your tests"},{"location":"/migrations/v0.7.0-Migration-Guide.html#migration-guide","text":"The following section will detail errors you may encounter while migrating from scio 0.6.x to Scio 0.7.x, and help you fix them. If you’ve run the automated migration fixes, you can jump directly to the Add missing context bounds section.","title":"Migration guide"},{"location":"/migrations/v0.7.0-Migration-Guide.html#method-xxx-is-not-a-member-of-com-spotify-scio-sciocontext","text":"When using read methods from ScioContext the compiler may issue an error of type method xxx is not a member of com.spotify.scio.ScioContext.\nIOs have been refactored in Scio 0.7.0. Each IO type now lives in the appropriate project and package. It means 2 things:","title":"method xxx is not a member of com.spotify.scio.ScioContext"},{"location":"/migrations/v0.7.0-Migration-Guide.html#you-may-need-to-explicitly-add-a-dependency-one-of-scios-subprojects-in-your-build-file","text":"For example, Scio used to pull dependencies on BigQuery IOs even if your pipeline did not use BigQuery at all. With the new IOs, Scio will limit its dependencies to packages you actually use.\nIf your pipeline is using BigQuery, you now need to add scio-bigquery as a dependency of your project:\nlibraryDependencies += \"com.spotify\" %% \"scio-bigquery\" % scioVersion","title":"You may need to explicitly add a dependency one of Scio’s subprojects in your build file"},{"location":"/migrations/v0.7.0-Migration-Guide.html#youll-need-to-import-the-appropriate-package-to-gain-access-to-the-io-methods-","text":"For example while migrating a job that reads data from an Avro file, you may see the following compiler error:\n[error] value avroFile is not a member of com.spotify.scio.ScioContext\n[error]     val coll = sc.avroFile[SomeType](uri)\n[error]                            ^\nAll you have to do to fix it is to import IOs from the correct package:\nimport com.spotify.scio.avro._","title":"You’ll need to import the appropriate package to gain access to the IO methods."},{"location":"/migrations/v0.7.0-Migration-Guide.html#avroio-or-other-type-of-io-not-found","text":"IOs have been moved out of the com.spotify.scio.testing package. To use them in unit tests (or elsewhere), you’ll need to change the import:\ncom.spotify.scio.testing.BigQueryIO -> com.spotify.scio.bigquery.BigQueryIO com.spotify.scio.testing.{AvroIO, ProtobufIO} -> com.spotify.scio.avro.{AvroIO, ProtobufIO} com.spotify.scio.testing.TextIO -> com.spotify.scio.io.TextIO\nA complete list of IO packages can be found here.\nAdditionally, some IOs are now parameterized. For example, AvroIO must now be parameterized with the Avro record type (either GenericRecord or an extension of SpecificRecordBase). In previous versions of Scio, it was possible in some cases to omit that type. For example:\nimport com.spotify.scio.io._\nimport com.spotify.scio.avro._\n// ...\nJobTest[MyScioJob.type]\n  .args(s\"--inputAUri=${inputAUri}\")\n  .args(s\"--inputBUri=${inputBUri}\")\n  .input(AvroIO[GenericRecord](inputAUri), Seq(Schemas.record1))\n  .input(AvroIO[GenericRecord](inputBUri), Seq(Schemas.record2))\n  .output(TextIO(output))(_ should haveSize (1))\n  .run()","title":"AvroIO (or other type of IO) not found"},{"location":"/migrations/v0.7.0-Migration-Guide.html#avro-type-inference-issue-","text":"If you use AvroIO you may see the compilation of your tests failing with an error looking like\n[error] <path>/SomeTest.scala:42:20: diverging implicit expansion for type com.spotify.scio.coders.Coder[K]\n[error] starting with macro method gen in trait LowPriorityCoderDerivation\n[error]       .input(AvroIO(inputUri), inputs)\n[error]\nThe problem is that line does not explicitly set the type of the IO:\n.input(AvroIO(inputUri), inputs)\nIn Scio <= 0.6.x this works, but in Scio 0.7.x, you’ll need to be explicit about the types. For example in that case:\n.input(AvroIO[GenericRecord](inputUri), inputs)","title":"Avro type inference issue: “diverging implicit expansion”"},{"location":"/migrations/v0.7.0-Migration-Guide.html#not-enough-arguments-for-method-top-topbykey-approxquantilesbykey-implicit-ord-ordering-","text":"Explicit Ordering functions for SCollection reducers are no longer curried. Methods that used to look like:\n.top(10)(Ordering.by(...)\nshould be changed to:\n.top(10, Ordering.by(...))","title":"Not enough arguments for method (top|topByKey|approxQuantilesByKey): (implicit ord: Ordering[…])"},{"location":"/migrations/v0.7.0-Migration-Guide.html#add-missing-context-bounds","text":"In the process of upgrading Scio, you may encounter the following error:\nCannot find a Coder instance for type T\nIf you’ve defined a generic function that uses a SCollection, this function is likely to need a Coder[T]. Scio will require you to provide an implicit Coder[T]. You can read about Scala implicit parameters here\nLet’s see a simple example. Say I created the following method doSomething:\ndef doSomething[T](coll: SCollection[T]): SCollection[T] =\n  coll.map { t =>\n    // do smth that returns a T\n  }\nIf I try to compile this method the compiler will return the following error:\nCannot find a Coder instance for type:\n\n  >> T\n\n  This can happen for a few reasons, but the most common case is that a data\n  member somewhere within this type doesn't have a Coder instance in scope. Here are\n  some debugging hints:\n    - For Option types, ensure that a Coder instance is in scope for the non-Option version.\n    - For List and Seq types, ensure that a Coder instance is in scope for a single element.\n    - You can check that an instance exists for Coder in the REPL or in your code:\n        scala> Coder[Foo]\n    And find the missing instance and construct it as needed.\n\n  coll.map { t =>\n           ^\nWhat this message says is that calling .map { ... } requires a Coder[T]. You can fix this very easily by adding a new implicit parameter to your method:\nimport com.spotify.scio.coders.Coder\n\ndef doSomething[T](coll: SCollection[T])(implicit coder: Coder[T]): SCollection[T] =\n  coll.map { t =>\n    // do smth that returns a T\n  }\nAlternatively, the same result can be achieved using Scala’s context bound syntax:\nimport com.spotify.scio.coders.Coder\n\ndef doSomething[T: Coder](coll: SCollection[T]): SCollection[T] =\n  coll.map { t =>\n    // do smth that returns a T\n  }","title":"Add missing context bounds"},{"location":"/migrations/v0.7.0-Migration-Guide.html#replacing-kryo-fallbacks-with-your-own-coders-","text":"Most of the time, the compiler will be able to find or derive an appropriate Coder automatically. Sometimes, it may not be able to find one automatically. This will typically happen for:\nClasses defined in Java Scala classes that are not case classes Classes with a private constructor\nIn those cases, Scio will fallback to using Kryo.","title":"Replacing Kryo fallbacks with your own coders."},{"location":"/migrations/v0.7.0-Migration-Guide.html#showing-all-fallback-at-compile-time","text":"The compiler can show a message each time a fallback is used. To activate that feature, just the the following scalac option: -Xmacro-settings:show-coder-fallback=true.\nYou can fix this warning in two ways:\nImplement a proper Coder for this type Make it explicit that the Kryo coder is in fact the one you want to use.\nIn both cases you want to define a Coder in your own code. The only difference is how you’ll implement it.\nLet’s say you are using a SCollection[java.util.Locale]:\ndef doSomething(coll: SCollection[String]): SCollection[java.util.Locale] =\n  coll.map { t =>\n    // do smth that returns a Locale\n  }","title":"Showing all fallback at compile time"},{"location":"/migrations/v0.7.0-Migration-Guide.html#using-kryo-explicitly","text":"If you want to explicitly use Kryo (which will probably be the case) you can do the following:\npackage myproject.coders\n\nimport java.util.Locale\nimport com.spotify.scio.coders.Coder\n\nobject Coders {\n  implicit val coderLocale: Coder[Locale] = Coder.kryo[Locale]\n}\nNow all you have to do is make that available at call site:\nimport myproject.coders.Coders._\n\ndef doSomething(coll: SCollection[String]): SCollection[java.util.Locale] =\n  coll.map { t =>\n    // do smth that returns a Locale\n  }","title":"Using Kryo explicitly"},{"location":"/migrations/v0.7.0-Migration-Guide.html#defining-a-custom-coder","text":"If you want to implement custom coders, see Scio’s source code for examples.\nWarning Before implementing custom coders, we recommend that you test your pipeline with the default coders. Implementing custom coders can be tricky, so make sure there’s a clear benefit in doing it. If you implement custom Coders, you need to make sure they are Serializable.","title":"Defining a custom Coder"},{"location":"/migrations/v0.7.0-Migration-Guide.html#wartremover-compatibility-","text":"Automatically derived coders are generated by a macro. Unfortunately, if you use WartRemover in your project, the macro will trigger warnings. There’s not much we can do in the macro to fix the issue right now, so you’ll have to disable a few warts. Here are the warts you’ll need to disable in your project:\n- Any\n- IsInstanceOf\n- Throw\nIf you use sbt-wartremover, you can disable them in your build like this:\nwartremoverErrors in (Compile, compile) := {\n  val disableWarts = Set(\n    Wart.Any,\n    Wart.IsInstanceOf,\n    Wart.Throw\n  )\n  Warts.unsafe.filterNot(disableWarts.contains)\n},\n\nwartremoverErrors in (Test, compile) := {\n  val disableWarts = Set(\n    Wart.Any,\n    Wart.IsInstanceOf,\n    Wart.Throw\n  )\n  Warts.unsafe.filterNot(disableWarts.contains)\n}","title":"WartRemover compatibility."},{"location":"/dev/index.html","text":"","title":"Development"},{"location":"/dev/index.html#development","text":"","title":"Development"},{"location":"/dev/Style-Guide.html","text":"","title":"Style Guide"},{"location":"/dev/Style-Guide.html#style-guide","text":"","title":"Style Guide"},{"location":"/dev/Style-Guide.html#general-guidelines","text":"","title":"General Guidelines"},{"location":"/dev/Style-Guide.html#scalafmt","text":"We use scalafmt to format code automatically. Run the following command to format the entire codebase.\nsbt scalafmt test:scalafmt scalafmtSbt","title":"Scalafmt"},{"location":"/dev/Style-Guide.html#intellij-idea","text":"Most of us write Scala code in IntelliJ IDEA and it’s wise to let the IDE do most of the work including managing imports and formatting code. We also want to avoid custom settings as much as possible to make on-boarding new developers easier. Hence we use IntelliJ IDEA’s default settings with the following exceptions:\nSet Code Style → Default Options → Right margin (columns) to 100 Turn off Code Style → Scala → ScalaDoc → Use scaladoc indent for leading asterisk Under Copyright → Copyright Profiles, add the following template.\nCopyright $today.year Spotify AB.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.","title":"IntelliJ IDEA"},{"location":"/dev/Style-Guide.html#scalastyle","text":"We use ScalaStyle to cover the basics and the entire code base should pass. In case of exceptions, wrap the violating code with a pair of comments to temporarily suppress the warning.\n// scalastyle:off regex\nprintln(\"hello\")\n// scalastyle:on regex","title":"ScalaStyle"},{"location":"/dev/Style-Guide.html#consistency","text":"There are many different ways to write Scala code and it’s hard to enforce a strict style. ScalaStyle’s rule set is also not as comprehensive as Java’s CheckStyle. When in doubt, be consistent with the rest of the code base.","title":"Consistency"},{"location":"/dev/Style-Guide.html#references","text":"We want to adhere to the styles of well known Scala projects and use the following documents as references. We follow the Databricks Scala Guide mainly with a few differences described in the next section.\nDatabricks Scala Guide The Official Scala Style Guide Twitter’s Effective Scala","title":"References"},{"location":"/dev/Style-Guide.html#differences-from-databricks-scala-guide","text":"","title":"Differences from Databricks Scala Guide"},{"location":"/dev/Style-Guide.html#spacing-and-indentation","text":"For method declarations, align parameters when they don’t fit in a single line. Return types can be either on the same line as the last parameter, or put to next line with no indent.\ndef saveAsBigQuery(table: TableReference, schema: TableSchema,\n                   writeDisposition: WriteDisposition,\n                   createDisposition: CreateDisposition)\n                  (implicit ev: T <:< TableRow): Future[Tap[TableRow]] = {\n  // method body\n}\n\ndef saveAsBigQuery(table: TableReference, schema: TableSchema,\n                   writeDisposition: WriteDisposition,\n                   createDisposition: CreateDisposition)\n                  (implicit ev: T <:< TableRow)\n: Future[Tap[TableRow]] = {\n  // method body\n}\nFor classes whose header doesn’t fit in a single line, align the next line and add a blank line after class header.\nclass Foo(val param1: String,\n          val param2: String,\n          val param3: Array[Byte])\n  extends FooInterface  // 2 space indent here\n  with Logging {\n\n  def firstMethod(): Unit = { ... }  // blank line above\n\n}","title":"Spacing and Indentation"},{"location":"/dev/Style-Guide.html#blank-lines-vertical-whitespace-","text":"A single blank line appears: Between consecutive members (or initializers) of a class: fields, constructors, methods, nested classes, static initializers, instance initializers. Within method bodies, as needed to create logical groupings of statements. Before the first member and after the last member of the class. A blank line is optional: Between consecutive one-liner fields or methods of a class that have no ScalaDoc. Before the first member and after the last member of a short class with one-liner members only. Use one blank line to separate class definitions. Excessive number of blank lines is discouraged.\nclass Foo {\n\n  val x: Int  // blank line before the first member\n  val y: Int\n  val z: Int  // no blank line between one-liners that have no ScalaDoc\n\n  def hello(): {\n    // body\n  }  // blank line after the last member\n\n}\n\n// no blank line before the first member and after the last member\nclass Bar {\n  def x = { /* body */ }\n  def y = { /* body */ }\n}","title":"Blank Lines (Vertical Whitespace)"},{"location":"/dev/Style-Guide.html#curly-braces","text":"Put curly braces even around one-line conditional or loop statements. The only exception is if you are using if/else as an one-line ternary operator that is also side-effect free.\n// the only exception for omitting braces\nval x = if (true) expression1 else expression2","title":"Curly Braces"},{"location":"/dev/Style-Guide.html#documentation-style","text":"Use Java docs style instead of Scala docs style. One-liner ScalaDoc is acceptable. Annotations like @tparam, @param, @return are optional if they are obvious to the user.\nScalaDoc /** */ should only be used for documenting API to end users. Use regular comments e.g. /* */ and // for explaining code to developers.\n/** This is a correct one-liner, short description. */\n\n/**\n * This is correct multi-line JavaDoc comment. And\n * this is my second line, and if I keep typing, this would be\n * my third line.\n */\n\n/** In Spark, we don't use the ScalaDoc style so this\n  * is not correct.\n  */\n\n// @param xs, @tparam T and @return are obvious and no need to document\n/** Sum up a sequence with an Algebird Semigroup. */\ndef sum[T: Semigroup](xs: Seq[T]): T = xs.reduce(implicitly[Semigroup[T]].plus)","title":"Documentation Style"},{"location":"/dev/Style-Guide.html#ordering-within-a-class","text":"If a class is long and has many methods, group them logically into different sections, and use comment headers to organize them.\nclass ScioContext {\n\n  // =======================================================================\n  // Read operations\n  // =======================================================================\n\n  // =======================================================================\n  // Accumulators\n  // =======================================================================\n\n}\nOf course, the situation in which a class grows this long is strongly discouraged, and is generally reserved only for building certain public APIs.","title":"Ordering within a Class"},{"location":"/dev/Style-Guide.html#imports","text":"Mutable collections Always prefix a mutable collection type with M when importing, e.g. import scala.collection.mutable.{Map => MMap} Or import scala.collection.mutable package and use mutable.Map Sort imports in IntelliJ IDEA’s default order: java.* All other imports scala.* It should look like this in Imports → Import Layout\njava\n_______ blank line _______\nall other imports\n_______ blank line _______\nscala","title":"Imports"},{"location":"/dev/How-to-Release.html","text":"","title":"How to Release"},{"location":"/dev/How-to-Release.html#how-to-release","text":"","title":"How to Release"},{"location":"/dev/How-to-Release.html#prerequisites","text":"Sign up for a Sonatype account here Ask for permissions to push to com.spotify domain like in this ticket Add Sonatype credentials to ~/.sbt/1.0/credentials.sbt credentials ++= Seq(\n  Credentials(\n\"Sonatype Nexus Repository Manager\",\n\"oss.sonatype.org\",\n\"$USERNAME\",\n\"$PASSWORD\"))\n Create a PGP key, for example on keybase.io, and distribute it to a public keyserver","title":"Prerequisites"},{"location":"/dev/How-to-Release.html#release-procedure","text":"Run the slow integration tests with SLOW=true sbt it:test Run release skip-tests in sbt console and follow the instructions Go to oss.sonatype.org, find the staging repository, “close” and “release” Run ./scripts/make-site.sh to update documentation Pick a release name from [here](https://en.wikipedia.org/wiki/List_of_Latin_phrases_(full)), here or other interesting sources* Update the list of release names below When the tag build completes, update release notes with name and change log If the release includes a Beam version bump, update the version matrix Run scripts/bump_scio.sh to update homebrew formula and scioVersion in downstream repos including scio.g8, scio-contrib, featran, etc. Bump version in the internal scio-cookie and monorepo Send internal announcement to scio-users@spotify.com and flatmap-announce@spotify.com Send external announcement to scio-users@googlegroups.com and user@beam.apache.org Announce on internal Slack and public Gitter room Announce on Twitter\n*Starting with 0.4.0 all release names are scientific names of animals with genus and species starting with the same letter, in ascending alphabetical order. Here are some sources of inspiration.\nhttp://www.animalbase.org/ https://a-z-animals.com/animals/scientific/ https://en.wikipedia.org/wiki/List_of_mammal_genera https://lib2.colostate.edu/wildlife/atoz.php","title":"Release procedure"},{"location":"/dev/How-to-Release.html#past-release-names","text":"","title":"Past release names"},{"location":"/dev/How-to-Release.html#0-7-x","text":"v0.7.0 - “Suricata suricatta”","title":"0.7.x"},{"location":"/dev/How-to-Release.html#0-6-x","text":"v0.6.1 - “Rhyncholestes raphanurus” v0.6.0 - “Quelea Quelea”","title":"0.6.x"},{"location":"/dev/How-to-Release.html#0-5-x","text":"v0.5.7 - “Panthera pardus” v0.5.6 - “Orcinus orca” v0.5.5 - “Nesolagus netscheri” v0.5.4 - “Marmota monax” v0.5.3 - “Lasiorhinus latifrons” v0.5.2 - “Kobus kob” v0.5.1 - “Jaculus jerboa” v0.5.0 - “Ia io”","title":"0.5.x"},{"location":"/dev/How-to-Release.html#0-4-x","text":"v0.4.7 - “Hydrochoerus hydrochaeris” v0.4.6 - “Galago gallarum” v0.4.5 - “Felis ferus” v0.4.4 - “Erinaceus europaeus” v0.4.3 - “Dendrohyrax dorsalis” v0.4.2 - “Castor canadensis” v0.4.1 - “Blarina brevicauda” v0.4.0 - “Atelerix albiventris”","title":"0.4.x"},{"location":"/dev/How-to-Release.html#0-3-x","text":"v0.3.6 - “Veritas odit moras” v0.3.5 - “Unitas, veritas, carnitas” v0.3.4 - “Sectumsempra” v0.3.3 - “Petrificus totalus” v0.3.2 - “Ut tensio sic vis” v0.3.1 - “Expecto patronum” v0.3.0 - “Lux et veritas”","title":"0.3.x"},{"location":"/dev/How-to-Release.html#0-2-x","text":"v0.2.13 - “Ex luna scientia” v0.2.12 - “In extremo” v0.2.11 - “Saltatio mortis” v0.2.10 - “De Mysteriis Dom Sathanas” v0.2.9 - “Hoc tempore atque nunc et semper” v0.2.8 - “Consummatum est” v0.2.7 - “Crescat scientia vita excolatur” v0.2.6 - “Sensu lato” v0.2.5 - “Imperium in imperio” v0.2.4 - “Ab imo pectore” v0.2.3 - “Aurea mediocritas” v0.2.2 - “Intelligenti pauca” v0.2.1 - “Sedes incertae” v0.2.0 - “Nulli secundus”","title":"0.2.x"},{"location":"/dev/How-to-Release.html#0-1-x","text":"v0.1.11 - “In silico” v0.1.10 - “Memento vivere” v0.1.9 - “Lucem sequimur” v0.1.8 - “Nemo saltat sobrius” v0.1.7 - “Spem gregis” v0.1.6 - “Sic infit” v0.1.5 - “Ad astra” v0.1.4 - “Ad arbitrium” v0.1.3 - “Ut cognoscant te” v0.1.2 - “Sapere aude” v0.1.1 - “Festina lente” v0.1.0 - “Scio me nihil scire”","title":"0.1.x"},{"location":"/dev/Design-Philosophy.html","text":"","title":"Design Philosophy"},{"location":"/dev/Design-Philosophy.html#design-philosophy","text":"We learned a lot building and improving Scio. The project was inspired by Spark and Scalding from the beginning, and we improved it over time working with customers of diverse background, including backend, data and ML. The design philosophy behind Scio can be summarized in a few points.\nMake it easy to do the right thing Scala made this possible for the most part. We have a fluent API and it’s easy to find the right transformation without going through lengthy documentation or source code. The most obvious thing is usually the best. .countByValue is clearer and more efficient than .map((_, 1L)).sumByKey than .map((_, 1L)).reduceByKey(_+_). Case classes and Options are much safer and easier than JSON-based TableRows with Objects and nulls, despite the effort we went through to make it work. One can .sum types with built-in Semigroups easily and correctly. Conversely there is no .groupAll since it could incur huge performance penalty and is essentially .groupBy(_ => ()). It’s easier to ask than making the wrong assumption and use it wrong (_“because it’s there”_). Make common use cases simple We have syntactic sugar for most common IO modules e.g. ScioContext#textFile, SCollection#saveAsBigQuery but don’t cover all possible parameters. There’s a trade-off between covering more use cases and keeping the API simple. We opted for a more flexible boilerplate free Args instead of the more type-safe PipelineOptions for command line arguments parsing. Mistakes in these parts of the code are easier to catch and less damaging than those in the computation logic. Another trade-off we made. We have syntactic sugars for various types of joins (hash, inner, outer, sketch) and side input operations (cross, lookup) that can be easily swapped to fine tune a pipeline. Make complex use cases possible We wrap complex internal APIs but don’t hide them away from users. Most low level Beam APIs (Pipeline, PCollection, PTransform) are still easily accessible. There are shorthands for integrating native Beam API, e.g. ScioContext#customInput, SCollection#saveAsCustomOutput, SCollection#applyTransform. Pipelines can be submitted from main, another process, a backend service, or chained with Futures.","title":"Design Philosophy"},{"location":"/scaladoc.html","text":"","title":""},{"location":"/Scio,-Beam-and-Dataflow.html","text":"","title":"Scio, Beam and Dataflow"},{"location":"/Scio,-Beam-and-Dataflow.html#scio-beam-and-dataflow","text":"Check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts. Also see this comparison between Scio, Scalding and Spark APIs.\nScio aims to be a thin wrapper on top of Beam while offering idiomatic Scala style API.","title":"Scio, Beam and Dataflow"},{"location":"/Scio,-Beam-and-Dataflow.html#basics","text":"ScioContext wraps Pipeline SCollection wraps PCollection ScioResult wraps PipelineResult Most PTransform are implemented as idiomatic Scala methods on SCollection e.g. map, flatMap, filter, reduce. PairSCollectionFunctions and DoubleSCollectionFunctions are specialized version of SCollection implemented via the Scala “pimp my library” pattern. An SCollection[(K, V)] is automatically converted to a PairSCollectionFunctions which provides key-value operations, e.g. groupByKey, reduceByKey, cogroup, join. An SCollection[Double] is automatically converted to a DoubleSCollectionFunctions which provides statistical operations, e.g. stddev, variance.","title":"Basics"},{"location":"/Scio,-Beam-and-Dataflow.html#sciocontext-pipelineoptions-args-and-scioresult","text":"Beam/Dataflow uses PipelineOptions and its subclasses to parse command line arguments. Users have to extend the interface for their application level arguments. Scalding uses Args to parse application arguments in a more generic and boilerplate free style. ScioContext has a parseArguments method that takes an Array[String] of command line arguments, parses Beam/Dataflow specific ones into a PipelineOptions, and application specific ones into an Args, and returns the (PipelineOptions, Args). ContextAndArgs is a short cut to create a (ScioContext, Args). ScioResult can be used to access accumulator values and job state.","title":"ScioContext, PipelineOptions, Args and ScioResult"},{"location":"/Scio,-Beam-and-Dataflow.html#io","text":"Most IO Read transforms are implemented as methods on ScioContext, e.g. avroFile, textFile, bigQueryTable. Most IO Write transforms are implemented as methods on SCollection, e.g. saveAsAvroFile, saveAsTextFile, saveAsBigQueryTable. These IO operations also detects when the ScioContext is running in a JobTest and manages test IO in memory. Write options also return a Future[Tap[T]] where Tap abstracts away the logic of reading the dataset directly as an Iterator[T] or re-opening it in another ScioContext. The Future is complete once the job finishes. This can be used to do light weight pipeline orchestration e.g. WordCountOrchestration.scala.","title":"IO"},{"location":"/Scio,-Beam-and-Dataflow.html#bykey-operations","text":"Beam/Dataflow ByKey transforms require PCollection[KV[K, V]] inputs while Scio uses SCollection[(K, V)] Hence every ByKey transform in PairSCollectionFunctions converts Scala (K, V) to KV[K, V] before and vice versa afterwards. However these are lightweight wrappers and the JVM should be able to optimize them. PairSCollectionFunctions also converts java.lang.Iterable[V] and java.util.List[V] to scala.Iterable[V] in some cases.","title":"ByKey operations"},{"location":"/Scio,-Beam-and-Dataflow.html#coders","text":"Beam/Dataflow uses Coder for (de)serializing elements in a PCollection during shuffle. There are built-in coders for Java primitive types, collections, and common types in GCP like Avro, ProtoBuf, BigQuery TableRow, Datastore Entity. PCollection uses TypeToken from Guava reflection and to workaround Java type erasure and retrieve type information of elements. This may not always work but there is a PCollection#setCoder method to override. Twitter’s chill library uses kryo to (de)serialize data. Chill includes serializers for common Scala types and cal also automatically derive serializers for arbitrary objects. Scio falls back to KryoAtomicCoder when a built-in one isn’t available. A coder may be non-deterministic if Coder#verifyDeterministic throws an exception. Any data type with such a coder cannot be used as a key in ByKey operations. However KryoAtomicCoder assumes all types are deterministic for simplicity so it’s up to the user’s discretion to not avoid non-deterministic types e.g. tuples or case classes with doubles as keys. Avro GenericRecord requires a schema during deserialization (which is available as GenericRecord#getSchema for serialization) and AvroCoder requires that too during initialization. This is not possible in KryoAtomicCoder, i.e. when nesting GenericRecord inside a Scala type. Instead KryoAtomicCoder serializes the schema before every record so that they can roundtrip safely. This is not optimal but the only way without requiring user to handcraft a custom coder.","title":"Coders"},{"location":"/Scio,-Scalding-and-Spark.html","text":"","title":"Scio, Spark and Scalding"},{"location":"/Scio,-Scalding-and-Spark.html#scio-spark-and-scalding","text":"Check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts. Also read more about the relationship between Scio, Beam and Dataflow.\nScio’s API is heavily influenced by Spark with a lot of ideas from Scalding.","title":"Scio, Spark and Scalding"},{"location":"/Scio,-Scalding-and-Spark.html#scio-and-spark","text":"The Dataflow programming model is fundamentally different from that of Spark. Read this Google blog article for more details.\nThe Scio API is heavily influenced by Spark but there are some minor differences.\nSCollection is equivalent to Spark’s RDD. PairSCollectionFunctions and DoubleSCollectionFunctions are specialized versions of SCollection and equivalent to Spark’s PairRDDFunctions and DoubleRDDFunctions. Execution planning is static and happens before the job is submitted. There is no driver node in a Dataflow cluster and one can only perform the equivalent of Spark transformations (RDD → RDD) but not actions (RDD → driver local memory). There is no broadcast either but the pattern of RDD → driver via action and driver → RDD via broadcast can be replaced with SCollection.asSingletonSideInput and SCollection.withSideInputs. There is no DStream (continuous series of RDDs) like in Spark Streaming. Values in a SCollection are windowed based on timestamp and windowing operation. The same API works regardless of batch (single global window by default) or streaming mode. Aggregation type transformations that produce SCollections of a single value under global window will produce one value each window when a non-global window is defined. SCollection has extra methods for side input, side output, and windowing.","title":"Scio and Spark"},{"location":"/Scio,-Scalding-and-Spark.html#scio-and-scalding","text":"Scio has a much simpler abstract data types compared to Scalding.\nScalding has many abstract data types like TypedPipe, Grouped, CoGrouped, SortedGrouped. Many of them are intermediate and enable some optimizations or wrap around Cascading’s data model. As a result many Scalding operations are lazily evaluated, for example in pipe.groupBy(keyFn).reduce(mergeFn), mergeFn is lifted into groupBy to operate on the map side as well. Scio on the other hand, has only one main data type SCollection[T] and SCollection[(K, V)] is a specialized variation when the elements are key-value pairs. All Scio operations are strictly evaluated, for example p.groupBy(keyFn) returns (K, Iterable[T]) where the values are immediately grouped, whereas p.reduceByKey(_ + _) groups (K, V) pairs on K and reduces values.\nSome features may look familiar to Scalding users.\nArgs is a simple command line argument parser similar to the one in Scalding. Powerful transforms are possible with sum, sumByKey, aggregate, aggregrateByKey using Algebird Semigroups and Aggregators. MultiJoin and coGroup of up to 22 sources. JobTest for end to end pipeline testing.","title":"Scio and Scalding"},{"location":"/Scio,-Scalding-and-Spark.html#scollection","text":"SCollection has a few variations.\nSCollectionWithSideInput for replicating small SCollections to all left-hand side values in a large SCollection. SCollectionWithSideOutput for output to multiple SCollections. WindowedSCollection for accessing window information. SCollectionWithFanout and SCollectionWithHotKeyFanout for fanout of skewed data.","title":"SCollection"},{"location":"/Scio,-Scalding-and-Spark.html#additional-features","text":"Scio also offers some additional features.\nEach worker can pull files from Google Cloud Storage via DistCache to be used in transforms locally, similar to Hadoop distributed cache. See DistCacheExample.scala. Type safe BigQuery IO via Scala macros. Case classes and converters are generated at compile time based on BQ schema. This eliminates the error prone process of handling generic JSON objects. See TypedBigQueryTornadoes.scala. Sinks (saveAs* methods) return Future[Tap[T]] that can be opened either in another pipeline as SCollection[T] or directly as Iterator[T] once the current pipeline completes. This enables complex pipeline orchestration. See WordCountOrchestration.scala.","title":"Additional features"},{"location":"/Runners.html","text":"","title":"Runners"},{"location":"/Runners.html#runners","text":"","title":"Runners"},{"location":"/Runners.html#runner-dependency","text":"Starting Scio 0.4.4, Beam runner is completely decoupled from scio-core, which no longer depend on any Beam runner now. Add runner dependencies to enable execution on specific backends. For example, when using Scio 0.4.7 which depends on Beam 2.2.0, you should add the following dependencies to run pipelines locally and on Google Cloud Dataflow.\nlibraryDependencies ++= Seq(\n  \"org.apache.beam\" % \"beam-runners-direct-java\" % \"2.2.0\",\n  \"org.apache.beam\" % \"beam-runners-google-cloud-dataflow-java\" % \"2.2.0\"\n)","title":"Runner dependency"},{"location":"/Runners.html#runner-specific-logic","text":"Dataflow specific logic, e.g. job ID, metrics, were also removed from ScioResult. You can convert between the generic ScioResult and runner specific result types like the example below. Note that currently only DataflowResult is implemented.\n// Generic result only\nval scioResult: ScioResult = sc.close()\n\n// Convert to Dataflow specific result\nimport com.spotify.scio.runners.dataflow._\nval dfResult: DataflowResult = scioResult.as[DataflowResult]\n\n// Convert back to generic result\nval scioResult2: ScioResult = dfResult.asScioResult\nGiven the Google Cloud project ID and Dataflow job ID, one can also create DataflowResult and ScioResult without running a pipeline. This could be when submitting jobs asynchronously and retrieving metrics later.\nval dfResult = DataflowResult(\"<PROJECT_ID>\", \"<JOB_ID>\")\nval scioResult = dfResult.asScioResult","title":"Runner specific logic"},{"location":"/Scio-data-guideline.html","text":"","title":"Data Guidelines"},{"location":"/Scio-data-guideline.html#data-guidelines","text":"Here are some common guidelines for building efficient, cost-effective, and maintainable pipelines. They apply to most use cases but you might have to tweak based on your needs. Also see the FAQ page for some common performance issues and remedies.","title":"Data Guidelines"},{"location":"/Scio-data-guideline.html#development","text":"Use Scio REPL to get familiar with Scio and perform ad-hoc experiments, but don’t use it as a replacement for unit tests.","title":"Development"},{"location":"/Scio-data-guideline.html#storage","text":"Leverage BigQuery, especially BigQuery SELECT query as input whenever possible. BigQuery has a very efficient columnar storage engine, can scale independently from Scio/Dataflow clusters and probably cheaper and easier to write than handcrafted Scala/Java pipeline code. Use BigQuery as an intermediate storage, especially if downstream jobs do a lot of slicing and dicing on rows and columns. Feel free to de-normalize data and use wide rows. Use Bigtable or Datastore depending on requirements for serving pipeline output to production services.","title":"Storage"},{"location":"/Scio-data-guideline.html#computation","text":"Prefer combine/aggregate/reduce transforms over groupByKey. Keep in mind that a reduce operation must be associative and commutative. Prefer sum/sumByKey over reduce/reduceByKey for basic data types. They use Algebird Semigroups and are often more optimized than hard coded reduce functions. See AlgebirdSpec.scala for more examples. Understand the performance characteristics of different types of joins and the role of side input cache, see the FAQ for more. Understand the Kryo serialization tuning options and use custom Kryo serializers for objects in the critical pass if necessary.","title":"Computation"},{"location":"/Scio-data-guideline.html#execution-parameters","text":"When tuning pipeline execution parameters, start with smaller workerMachineType e.g. default n1-standard-1 to n1-standard-4, and reasonable maxNumWorkers that reflect your input size. Keep in mind that there might be limited availability of large GCE instances and more workers means higher shuffle cost.","title":"Execution parameters"},{"location":"/Scio-data-guideline.html#streaming","text":"For streaming jobs with periodically updated state, i.e. log decoration with metadata, keep (and update) states in Bigtable, and do look ups from the streaming job (read more about Bigtable key structure). Also see BigtableDoFn for an abstraction that handles asynchronous Bigtable requests. For streaming, larger worker machine types and SSD for workerDiskType might be more suitable. A typical job with 5 x n1-standard-4 and 100GB SSDs can handle ~30k peak events per second. Also see this article on disk performance.","title":"Streaming"},{"location":"/Apache-Beam.html","text":"","title":"Versions"},{"location":"/Apache-Beam.html#versions","text":"Starting from version 0.3.0, Scio moved from Google Cloud Dataflow Java SDK to Apache Beam as its core dependencies and introduced a few breaking changes.\nDataflow Java SDK 1.x.x uses com.google.cloud.dataflow.sdk namespace. Apache Beam uses org.apache.beam.sdk namespace. Dataflow Java SDK 2.x.x is also based on Apache Beam 2.x.x and uses org.apache.beam.sdk.\nScio 0.3.x depends on Beam 0.6.0 (last pre-stable release) and Scio 0.4.x depends on Beam 2.0.0 (first stable release). Breaking changes in these releases are documented below.","title":"Versions"},{"location":"/Apache-Beam.html#version-matrices","text":"Early Scio releases depend on Google Cloud Dataflow Java SDK while later ones depend on Apache Beam. Check out the Changelog page for migration guides.\nScio SDK Dependency Description 0.7.x Apache Beam 2.x.x Static coders, new ScioIO 0.6.x Apache Beam 2.x.x Cassandra 2.2 0.5.x Apache Beam 2.x.x Better type-safe Avro and BigQuery IO 0.4.x Apache Beam 2.0.0 Stable release Beam 0.3.x Apache Beam 0.6.0 Pre-release Beam 0.2.x Dataflow Java SDK SQL-2011 support 0.1.x Dataflow Java SDK First releases\nScio Version Beam Version 0.7.0 2.9.0 0.6.0 2.6.0 0.5.7 2.6.0 0.5.6 2.5.0 0.5.1+ 2.4.0 0.5.0 2.2.0 0.4.6+ 2.2.0 0.4.1+ 2.1.0 0.4.0 2.0.0 0.3.0+ 0.6.0","title":"Version matrices"},{"location":"/Apache-Beam.html#release-cycle-and-backport-procedures","text":"Scio has a frequent release cycle, roughly every 2-4 weeks, as compared to months for the upstream Apache Beam. We also aim to stay a step ahead by pulling changes from upstream and contributing new ones back.\nLet’s call the Beam version that Scio depends on current, and upstream master latest. Here’re the procedures for backporting changes.\nFor changes available in latest but not in current: - Copy Java files from latest to Scio repo - Rename classes and modify as necessary - Release Scio - Update checklist for the next current version like #633 - Remove change once current is updated\nFor changes we want to make to latest: - Submit pull request to latest - Follow the steps above once merged","title":"Release cycle and backport procedures"},{"location":"/Apache-Beam.html#beam-master-nightly-build","text":"To keep up with upstream changes, beam-master branch is built nightly and depends on latest Beam SNAPSHOT.\nWe should do the following periodically to reduce work when upgrading Beam release version. - rebase beam-master on master - fix for breaking changes in beam-master - rebase master on beam-master when upgrading Beam release version.\nTo work on a breaking change: - checkout beam-master branch - run ./scripts/circleci_snapshot.sh to change beamVersion to the latest SNAPSHOT - run sbt test it:test and fix errors","title":"Beam master nightly build"},{"location":"/Changelog.html","text":"","title":"Changelog"},{"location":"/Changelog.html#changelog","text":"","title":"Changelog"},{"location":"/Changelog.html#breaking-changes-since-scio-0-7-0-","text":"New Magnolia based Coders derivation New ScioIO replaces TestIO[T] to simplify IO implementation and stubbing in JobTest","title":"Breaking changes since Scio 0.7.0 (v0.7.0 Migration Guide)"},{"location":"/Changelog.html#breaking-changes-since-scio-0-6-0","text":"scio-cassandra2 now requires Cassandra 2.2 instead of 2.0","title":"Breaking changes since Scio 0.6.0"},{"location":"/Changelog.html#breaking-changes-since-scio-0-5-0","text":"BigQueryIO in JobTest now requires a type parameter which could be either TableRow for JSON or T for type-safe API where T is a type annotated with @BigQueryType. Explicit .map(T.toTableRow) of test data is no longer needed. See changes in BigQueryTornadoesTest and TypedBigQueryTornadoesTest for more. Typed AvroIO now accepts case classes instead of Avro records in JobTest. Explicit .map(T.toGenericRecord) of test data is no longer needed. See this change for more. Package com.spotify.scio.extra.transforms is moved from scio-extra to scio-core, under com.spotify.scio.transforms.","title":"Breaking changes since Scio 0.5.0"},{"location":"/Changelog.html#breaking-changes-since-scio-0-4-0","text":"Accumulators are replaced by the new metrics API, see MetricsExample.scala for more com.spotify.scio.hdfs package and related APIs (ScioContext#hdfs*, SCollection#saveAsHdfs*) are removed, regular file IO API should now support both GCS and HDFS (if scio-hdfs is included as a dependency). Starting Scio 0.4.4, Beam runner is completely decoupled from scio-core. See Runners page for more details.","title":"Breaking changes since Scio 0.4.0"},{"location":"/Changelog.html#breaking-changes-since-scio-0-3-0","text":"See this page for a list of breaking changes from Dataflow Java SDK to Beam Scala 2.10 is dropped, 2.11 and 2.12 are the supported Scala binary versions Java 7 is dropped and Java 8+ is required DataflowPipelineRunner is renamed to DataflowRunner DirectPipelineRunner is renamed to DirectRunner BlockingDataflowPipelineRunner is removed and ScioContext#close() will not block execution; use sc.close().waitUntilDone() to retain the blocking behavior, i.e. if you launch job from an orchestration engine like Airflow or Luigi You should set tempLocation instead of stagingLocation regardless of runner; set it to a local path for DirectRunner or a GCS path for DataflowRunner; if not set, DataflowRunner will create a default bucket for the project Type safe BigQuery is now stable API; use import com.spotify.scio.bigquery._ instead of import com.spotify.scio.experimental._ scio-bigtable no longer depends on HBase and uses Protobuf based Bigtable API; check out the updated example Custom IO, i.e. ScioContext#customInput and SCollection#saveAsCustomOutput require a name: String parameter","title":"Breaking changes since Scio 0.3.0"},{"location":"/FAQ.html","text":"","title":"FAQ"},{"location":"/FAQ.html#faq","text":"General questions What’s the status of Scio? Who’s using Scio? What’s the relationship between Scio and Apache Beam? What’s the relationship between Scio and Google Cloud Dataflow? How does Scio compare to Scalding or Spark? What are GCE availability zone and GCS bucket location? Programming questions How do I setup a new SBT project? How do I deploy Scio jobs to Dataflow? How do I use the SNAPSHOT builds of Scio? How do I unit test pipelines? How do I combine multiple input sources? How do I log in a job? How do I use Beam’s Java API in Scio? What are the different types of joins and performance implication? How to create Dataflow job template? How do I cancel a job after certain time period? Why can’t I have an SCollection inside another SCollection? BigQuery questions What is BigQuery dataset location? How stable is the type safe BigQuery API? How do I work with nested Options in type safe BigQuery? How do I unit test BigQuery queries? How do I stream to a partitioned BigQuery table? How do I invalidate cached BigQuery results or disable cache? How does BigQuery determines job priority? Streaming questions How do I update a streaming job? How do I read Pubsub input in a local pipeline? Other IO components How do I access various files outside of a ScioContext? How do I reduce Datastore boilerplate? How do I throttle Bigtable writes? How do I use custom Kryo serializers? What Kryo tuning options are there? Development environment issues How do I keep SBT from running out of memory? How do I fix “Unable to create parent directories” error in IntelliJ? How to make IntelliJ IDEA work with type safe BigQuery classes? Common issues What does “Cannot prove that T1 <:< T2” mean? How do I fix invalid default BigQuery credentials? Why are my typed BigQuery case classes not up to date? How do I fix “SocketTimeoutException” with BigQuery? Why do I see names like “main@{NativeMethodAccessorImpl...}” in the UI? How do I fix “RESOURCE_EXHAUSTED” error? Can I use “scala.App” trait instead of “main” method? How to inspect the content of an SCollection? How do I improve side input performance? How do I control concurrency (number of DoFn threads) in Dataflow workers How to manually investigate a Cloud Dataflow worker","title":"FAQ"},{"location":"/FAQ.html#general-questions","text":"","title":"General questions"},{"location":"/FAQ.html#whats-the-status-of-scio-","text":"Scio is widely being used for production data pipelines at Spotify and is our preferred framework for building new pipelines on Google Cloud. We run Scio on Google Cloud Dataflow service in both batch and streaming modes. However it’s still under heavy development and there might be minor breaking API changes from time to time.","title":"What’s the status of Scio?"},{"location":"/FAQ.html#whos-using-scio-","text":"Spotify uses Scio for all new data pipelines running on Google Cloud Platform, including music recommendation, monetization, artist insights and business analysis. We also use BigQuery, Bigtable and Datastore heavily with Scio. We use Scio in both batch and streaming mode.\nAs of mid 2017, there’re 200+ developers and 700+ production pipelines. The largest batch job we’ve seen uses 800 n1-highmem-32 workers (25600 CPUs, 166.4TB RAM) and processes 325 billion rows from Bigtable (240TB). We also have numerous jobs that process 10TB+ of BigQuery data daily. On the streaming front, we have many jobs with 30+ n1-standard-16 workers (480 CPUs, 1.8TB RAM) and SSD disks for real time machine learning or reporting.\nFor a incomplete list of users, see the Powered By page.","title":"Who’s using Scio?"},{"location":"/FAQ.html#whats-the-relationship-between-scio-and-apache-beam-","text":"Scio is a Scala API built on top of Apache Beam’s Java SDK. Scio aims to offer a concise, idiomatic Scala API for a subset of Beam’s features, plus extras we find useful, like REPL, type safe BigQuery, and IO taps.","title":"What’s the relationship between Scio and Apache Beam?"},{"location":"/FAQ.html#whats-the-relationship-between-scio-and-google-cloud-dataflow-","text":"Scio (version before 0.3.0) was originally built on top of Google Cloud Dataflow’s Java SDK. Google donated the code base to Apache and renamed it Beam. Cloud Dataflow became one of the supported runners, alongside Apache Flink & Apache Spark. Scio 0.3.x is built on top of Beam 0.6.0 and 0.4.x is built on top of Beam 2.x. Many users run Scio on the Dataflow runner today.","title":"What’s the relationship between Scio and Google Cloud Dataflow?"},{"location":"/FAQ.html#how-does-scio-compare-to-scalding-or-spark-","text":"Check out the wiki page on Scio, Scalding and Spark. Also check out Big Data Rosetta Code for some snippets.","title":"How does Scio compare to Scalding or Spark?"},{"location":"/FAQ.html#what-are-gce-availability-zone-and-gcs-bucket-location-","text":"GCE availability zone is where the Google Cloud Dataflow service spins up VM instances for your job, e.g. us-east1-a. Each GCS bucket (gs://bucket) has a storage class and bucket location that affects availability, latency and price. The location should be close to GCE availability zone. Dataflow uses --stagingLocation for job jars, temporary files and BigQuery I/O.","title":"What are GCE availability zone and GCS bucket location?"},{"location":"/FAQ.html#programming-questions","text":"","title":"Programming questions"},{"location":"/FAQ.html#how-do-i-setup-a-new-sbt-project-","text":"Read the documentation.","title":"How do I setup a new SBT project?"},{"location":"/FAQ.html#how-do-i-deploy-scio-jobs-to-dataflow-","text":"When developing locally, you can do sbt \"runMain MyClass ... or just runMain MyClass ... in the SBT console without building any artifacts.\nWhen deploying to the cloud, we recommend using sbt-pack or sbt-native-packager plugin instead of sbt-assembly. Unlike assembly, they pack dependency jars in a directory instead of merging them, so that we don’t have to deal with merge strategy and dependency jars can be cached by Dataflow service.\nAt Spotify we pack jars with sbt-pack, build docker images with sbt-docker together with orchestration components e.g. Luigi or Airflow and deploy them with Styx.","title":"How do I deploy Scio jobs to Dataflow?"},{"location":"/FAQ.html#how-do-i-use-the-snapshot-builds-of-scio-","text":"Commits to Scio master are automatically published to Sonatype via continuous integration. To use the latest SNAPSHOT artifact, add the following line to your build.sbt.\nresolvers += Resolver.sonatypeRepo(\"snapshots\")\nOr you can configure SBT globally by adding the following to ~/.sbt/1.0/global.sbt.\nresolvers ++= Seq(\n  Resolver.sonatypeRepo(\"snapshots\")\n  // other resolvers\n)","title":"How do I use the SNAPSHOT builds of Scio?"},{"location":"/FAQ.html#how-do-i-unit-test-pipelines-","text":"Any Scala or Java unit testing frameworks can be used with Scio but we provide some utilities for ScalaTest.\nPipelineTestUtils - utilities for testing parts of a pipeline JobTest - for testing pipelines end-to-end with complete arguments and IO coverage SCollectionMatchers - ScalaTest matchers for SCollection PipelineSpec - shortcut for ScalaTest FlatSpec with utilities and matchers\nThe best place to find example useage of JobTest and SCollectionMatchers are their respective tests in JobTestTest and SCollectionMatchersTest. For more examples see:\nscio-examples https://github.com/spotify/big-data-rosetta-code/tree/master/src/test/scala/com/spotify/bdrc/testing","title":"How do I unit test pipelines?"},{"location":"/FAQ.html#how-do-i-combine-multiple-input-sources-","text":"How do I combine multiple input sources, e.g. different BigQuery tables, files located in different GCS buckets? You can combine SCollections from different sources into one using the companion method SCollection.unionAll, for example:\nval (sc, args) = ContextAndArgs(cmdlineArgs)\n\nval collections = Seq(\"gs://bucket1/data/*.avro\", \"gs://bucket2/data/*.avro\")\n    .map(sc.avroFile[SchemaType](_))\nval all = SCollection.unionAll(collections)","title":"How do I combine multiple input sources?"},{"location":"/FAQ.html#how-do-i-log-in-a-job-","text":"You can log in a Scio job with most common logging libraries but slf4j is included as a dependency. Define the logger instance as a member of the job object and use it inside a lambda.\nimport org.slf4j.LoggerFactory\nobject MyJob {\n  private val logger = LoggerFactory.getLogger(this.getClass)\n  def main(cmdlineArgs: Array[String]): Unit = {\n    // ...\n    sc.parallelize(1 to 100)\n      .map { i =>\n        logger.info(s\"Element $i\")\n        i * i\n      }\n    // ...\n  }\n}","title":"How do I log in a job?"},{"location":"/FAQ.html#how-do-i-use-beams-java-api-in-scio-","text":"Scio exposes a few things to allow easy integration with native Beam Java API, notably:\nScioContext#customInput to apply a PTransform[_ >: PBegin, PCollection[T]] (source) and get a SCollection[T]. SCollection#applyTransform to apply a PTransform[_ >: PCollection[T], PCollection[U]] and get a SCollection[U] SCollection#saveAsCustomOutput to apply a PTransform[_ >: PCollection[T], PDone] (sink) and get a Future[Tap[T]].\nSee BeamExample.scala for more details. Custom I/O can also be tested via the JobTest harness.","title":"How do I use Beam’s Java API in Scio?"},{"location":"/FAQ.html#what-are-the-different-types-of-joins-and-performance-implication-","text":"Inner (a.join(b)), left (a.leftOuterJoin(b)), outer (a.fullOuterJoin(b)) performs better with a large LHS. So a should be the larger data set with potentially more hot keys, i.e. key with many values. Every key-value pair from every input is shuffled. join/leftOuterJoin may be replaced by hashJoin/leftHashJoin if the RHS is small enough to fit in memory (e.g. < 1GB). The RHS is used as a multi-map side input for the LHS. No shuffle is performed. Consider skewedJoin if some keys on the LHS are extremely hot. Consider sparseOuterJoin if you want a full outer join where RHS is much smaller than LHS, but may not fit in memory. Consider cogroup if you need to access value groups of each key. MultiJoin supports inner, left, outer join and cogroup of up to 22 inputs. For multi-joins larger inputs should be on the left, e.g. size(a) >= size(b) >= size(c) >= size(d) in MultiJoin(a, b, c, d). Check out these slides for more information on joins. Also see this section on Cloud Dataflow Shuffle service.","title":"What are the different types of joins and performance implication?"},{"location":"/FAQ.html#how-to-create-dataflow-job-template-","text":"For Apache Beam based Scio (version >= 0.3.0) use DataflowRunner and specify templateLocation option. For example in CLI --templateLocation=gs://<bucket>/job1. Read more about templates here.","title":"How to create Dataflow job template?"},{"location":"/FAQ.html#how-do-i-cancel-a-job-after-certain-time-period-","text":"You can wait on the ScioResult and call the internal PipelineResult#cancel() method if a timeout exception happens.\nval r = sc.close()\nimport scala.concurrent.duration._\nif (Try(r.waitUntilFinish(1.minute)).isFailure) {\n  r.internal.cancel()\n}","title":"How do I cancel a job after certain time period?"},{"location":"/FAQ.html#why-cant-i-have-an-scollection-inside-another-scollection-","text":"You cannot have an SCollection inside another SCollection, i.e. anything with type SCollection[SCollection[T]]. To explain this we have to go back to the relationship between ScioContext and SCollection. Every ScioContext represents a unique pipeline and every SCollection represents a stage in the pipeline execution, i.e. the state of the pipeline after some transforms has be applied. We start a pipeline code with val sc = ..., create new SCollections with methods on sc, e.g. sc.textFile, and transform them with methods like .map, .filter, .join. Therefore each SCollection can trace its root to one single sc. The pipeline is submitted for execution when we call sc.close(). Hence we cannot have an SCollection inside another SCollection just as we cannot have a pipeline inside another pipeline.","title":"Why can’t I have an SCollection inside another SCollection?"},{"location":"/FAQ.html#bigquery-questions","text":"","title":"BigQuery questions"},{"location":"/FAQ.html#what-is-bigquery-dataset-location-","text":"Each BigQuery dataset has a location (e.g. US, EU) and every table inside are stored in the same location. Tables in a JOIN must be from the same region. Also one can only import/export tables to a GCS bucket in the same location. Starting from v0.2.1, Scio will detect the dataset location of a query and create a staging dataset for ScioContext#bigQuerySelect and @BigQueryType.fromQuery. This location should be the same as that of your --stagingLocation GCS bucket. The old -Dbigquery.staging_dataset.location flag is removed.\nBecause of these limitations and performance reasons, make sure --zone, --stagingLocation and -Dbigquery.staging_dataset.location location of BigQuery datasets are consistent.","title":"What is BigQuery dataset location?"},{"location":"/FAQ.html#how-stable-is-the-type-safe-bigquery-api-","text":"Type Safe BigQuery API is considered stable and widely used at Spotify. There are several caveats however:\nBoth legacy and SQL syntax are supported although the SQL syntax is highly recommended The system will detect legacy or SQL syntax and choose the correct one To override auto-detection, start the query with either #legacysql or #standardsql comment line Legacy syntax is less predictable, especially for complex queries and may be disabled in the future Case classes generated by @BigQueryType.fromTable or @BigQueryType.fromQuery are not recognized in IntelliJ IDEA, but see this section for a workaround","title":"How stable is the type safe BigQuery API?"},{"location":"/FAQ.html#how-do-i-work-with-nested-options-in-type-safe-bigquery-","text":"Any nullable field in BigQuery is translated to Option[T] by the type safe BigQuery API and it can be clunky to work with rows with multiple or nested fields. For example:\nif (row.getUser.isDefined) {  // Option[User]\n  val email = row.getUser.get.getEmail  // Option[String]\n  if (email.isDefined) {\n    doSomething(email.get)\n  }\n}\nFor comprehension is a nicer alternative in these cases:\nval e = for (u <- row.getUser; e <- u.getUser) yield e  // Option[String]\ne.foreach(doSomething)\nAlso see these slides and this blog article.","title":"How do I work with nested Options in type safe BigQuery?"},{"location":"/FAQ.html#how-do-i-unit-test-bigquery-queries-","text":"BigQuery doesn’t provide a way to unit test query logic locally, but we can query the service directly in an integration test. Take a look at BigQueryIT.scala. MockBigQuery will create temporary tables on the service, feed them with mock data, and substitute table references in your query string with the mocked ones.","title":"How do I unit test BigQuery queries?"},{"location":"/FAQ.html#how-do-i-stream-to-a-partitioned-bigquery-table-","text":"Currently there is no way to create a partitioned BigQuery table via Scio/Beam when streaming, however it is possible to stream to a partitioned table if it is already created.\nThis can be done by using fixed windows and using the window bounds to infer date. As of Scio 0.4.0-beta2 this looks as follows:\nclass DayPartitionFunction() extends SerializableFunction[ValueInSingleWindow[TableRow], TableDestination] {\n  override def apply(input: ValueInSingleWindow[TableRow]): TableDestination = {\n    val partition = DateTimeFormat.forPattern(\"yyyyMMdd\").withZone(DateTimeZone.UTC)\n      .print(input.getWindow.asInstanceOf[IntervalWindow].start())\n    new TableDestination(\"project:dataset.partitioned$\" + partition, \"\")\n  }\n}\n\nsc.pubsubSubscription(\"projects/data-university/topics/data-university\")\n  .withFixedWindows(30L)\n// Convert to `TableRow`\n  .map(myStringToTableRowConversion: String => TableRow)\n  .saveAsCustomOutput(\n    \"SaveAsDayPartitionedBigQuery\",\n    BigQueryIO.writeTableRows().to(\n      new DayPartitionFunction())\n      .withWriteDisposition(WriteDisposition.WRITE_APPEND)\n      .withCreateDisposition(CreateDisposition.CREATE_NEVER)\n  )\nIn Scio 0.3.X it is possible to achieve the same behaviour using SerializableFunction[BoundedWindow, String] and BigQueryIO.Write.to. It is also possible to stream to separate tables with a Date suffix by modifying DayPartitionFunction, specifying the Schema, and changing the CreateDisposition to CreateDisposition.CREATE_IF_NEEDED.","title":"How do I stream to a partitioned BigQuery table?"},{"location":"/FAQ.html#how-do-i-invalidate-cached-bigquery-results-or-disable-cache-","text":"Scio’s BigQuery client in Scio caches query result in system property bigquery.cache.directory, which defaults to $PWD/.bigquery. Use rm -rf .bigquery to invalidate all cached results. To disable caching, set system property bigquery.cache.enabled to false.","title":"How do I invalidate cached BigQuery results or disable cache?"},{"location":"/FAQ.html#how-does-bigquery-determines-job-priority-","text":"By default Scio runs BigQuery jobs with BATCH priority except when in the REPL where it runs with INTERACTIVE. To override this, set system property bigquery.priority to either BATCH or INTERACTIVE.","title":"How does BigQuery determines job priority?"},{"location":"/FAQ.html#streaming-questions","text":"","title":"Streaming questions"},{"location":"/FAQ.html#how-do-i-update-a-streaming-job-","text":"Dataflow allows streaming jobs to be updated on the fly by specifying --update, along with --jobName=[your_job] on the command line. See https://cloud.google.com/dataflow/pipelines/updating-a-pipeline for detailed docs. Note that for this to work, Dataflow needs to be able to identify which transformations from the original job map to those in the replacement job. The easiest way to do so is to give unique names to transforms in the code itself. In Scio, this can be achieved by calling .withName() before applying the transform. For example:\nsc.textFile(...)\n   .withName(\"MakeUpper\").map(_.toUpperCase)\n   .withName(\"BigWords\").filter(_.length > 6)\nIn this example, the map’s transform name is “MakeUpper” and the filter’s is “BigWords”. If we later decided that we want to count 6 letter words as “big” too, then we can change it to _.length > 5, and because the transform name is the same the job can be updated on the fly.","title":"How do I update a streaming job?"},{"location":"/FAQ.html#how-do-i-read-pubsub-input-in-a-local-pipeline-","text":"You can use a custom PubsubIO transform and specify maxNumRecord & maxReadTime in order not to blow up local JVM.\nsc.customInput(\"ReadFromPubsub\",\n  PubsubIO.read()\n    .topic(\"projects/data-university/topics/data-university\")\n    .idLabel(\"id\")\n    .timestampLabel(\"ts\")\n    .withCoder(StringUtf8Coder.of())\n    .maxNumRecords(50)\n    .maxReadTime(Duration.standardMinutes(10))","title":"How do I read Pubsub input in a local pipeline?"},{"location":"/FAQ.html#other-io-components","text":"","title":"Other IO components"},{"location":"/FAQ.html#how-do-i-access-various-files-outside-of-a-sciocontext-","text":"For Scio version >= 0.4.0\nStarting from Scio 0.4.0 you can use Apache Beam’s Filesystems abstraction:\n// the path can be any of the supported Filesystems, e.g. local, GCS, HDFS\nval readmeResource = FileSystems.matchNewResource(\"gs://<bucket>/README.md\")\nval readme = FileSystems.open(readmeResource)\nFor Scio version < 0.4.0\nNote This part is GCS specific.\nYou can get a GcsUtil instance from ScioContext, which can be used to open GCS files in read or write mode.\nval gcsUtil = sc.optionsAs[GcsOptions].getGcsUtil","title":"How do I access various files outside of a ScioContext?"},{"location":"/FAQ.html#how-do-i-reduce-datastore-boilerplate-","text":"Datastore Entity class is actually generated from Protobuf which uses the builder pattern and very boilerplate heavy. You can use the shapeless-datatype library to seamlessly convert bewteen case classes and Entitys. See ShapelessDatastoreExample.scala for an example job and ShapelessDatastoreExampleTest.scala for tests.","title":"How do I reduce Datastore boilerplate?"},{"location":"/FAQ.html#how-do-i-throttle-bigtable-writes-","text":"Currently Dataflow autoscaling may not work well with large writes BigtableIO. Specifically It does not take into account Bigtable IO rate limits and may scale up more workers and end up hitting the limit and eventually fail the job. As a workaround, you can enable throttling for Bigtable writes in Scio 0.4.0-alpha2 or later.\nval btOptions = new BigtableOptions.Builder()\n  .setProjectId(btProjectId)\n  .setInstanceId(btInstanceId)\n  .setBulkOptions(new BulkOptions.Builder()\n    .enableBulkMutationThrottling()\n    .setBulkMutationRpcTargetMs(10) // lower latency threshold, default is 100\n    .build())\n  .build()\ndata.saveAsBigtable(btOptions, btTableId)","title":"How do I throttle Bigtable writes?"},{"location":"/FAQ.html#how-do-i-use-custom-kryo-serializers-","text":"See Kryo for more.\nDefine a registrar class that extends IKryoRegistrar and annotate it with @KryoRegistrar. Note that the class name must ends with KryoRegistrar, i.e. MyKryoRegistrar for Scio to find it.\nimport com.twitter.chill._\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    // register serializers for additional classes here\n    k.forClass(new UserRecordSerializer)\n    k.forClass(new AccountRecordSerializer)\n    ...\n  }\n}\nRegistering just the classes can also improve Kryo performance. By registering, classes will be serialized as numeric IDs instead of fully qualified class names, hence saving space and network IO while shuffling.\nimport com.twitter.chill._\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    k.registerClasses(List(classOf[MyRecord1], classOf[MyRecord2]))\n  }\n}","title":"How do I use custom Kryo serializers?"},{"location":"/FAQ.html#what-kryo-tuning-options-are-there-","text":"See KryoOptions.java for a complete list of available Kryo tuning options. These can be passed via command line, for example:\n--kryoBufferSize=1024 --kryoMaxBufferSize=8192 --kryoReferenceTracking=false --kryoRegistrationRequired=true\nAmong these, --kryoRegistrationRequired=true might be useful when developing to ensure that all data types in the pipeline are registered.","title":"What Kryo tuning options are there?"},{"location":"/FAQ.html#development-environment-issues","text":"","title":"Development environment issues"},{"location":"/FAQ.html#how-do-i-keep-sbt-from-running-out-of-memory-","text":"SBT might run out of memory sometimes and show an OutOfMemoryError: Metaspace error. Override default memory setting with -mem <integer>, e.g. sbt -mem 1024.","title":"How do I keep SBT from running out of memory?"},{"location":"/FAQ.html#how-do-i-fix-error-in-intellij-","text":"You might get an error message like java.io.IOException: Unable to create parent directories of /Applications/IntelliJ IDEA CE.app/Contents/bin/.bigquery/012345abcdef.schema.json. This usually happens to people who run IntelliJ IDEA with its bundled JVM. There are two solutions.\nInstall JDK from java.com and switch to it by following the “All platforms: switch between installed runtimes” section in this page. Override the bigquery .cache directory as a JVM compiler parameter. On the bottom right of the IntelliJ window, click the icon that looks like a clock, and then “Configure…”. Then, edit the JVM parameters to include the line -Dbigquery.cache.directory=</path/to/repository>/.bigquery. Then, restart the compile server by clicking on the clock icon -> Stop, and then Start.","title":"How do I fix “Unable to create parent directories” error in IntelliJ?"},{"location":"/FAQ.html#how-to-make-intellij-idea-work-with-type-safe-bigquery-classes-","text":"Due to issue SCL-8834 case classes generated by @BigQueryType.fromTable or @BigQueryType.fromQuery are not recognized in IntelliJ IDEA. There are two workarounds. The first, IDEA plugin solution, is highly recommended.\nIDEA Plugin\nInside IntelliJ, Preferences -> Plugins -> Browse repositories ... and search Scio. Install the plugin, restart IntelliJ, recompile the project (use SBT or IntelliJ). You have to recompile the project each time you add/edit @BigQueryType macro. Plugin requires Scio >= 0.2.2. Documentation.\nUse case class from @BigQueryType.toTable\nFirst start Scio REPL and generate case classes from your query or table.\nscio> @BigQueryType.fromQuery(\"SELECT tornado, month FROM [publicdata:samples.gsod]\") class Tornado\ndefined class Tornado\ndefined object Tornado\nNext print Scala code of the generated classes.\nscio> Tornado.toPrettyString()\nres1: String =\n@BigQueryType.toTable\ncase class Tornado(tornado: Option[Boolean], month: Long)\nYou can then paste the @BigQueryType.toTable code into your pipeline and use it with sc.typedBigQuery.\n@BigQueryType.toTable\ncase class Tornado(tornado: Option[Boolean], month: Long)\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  // ...\n  sc.typedBigQuery[Tornado](\"SELECT tornado, month FROM [publicdata:samples.gsod]\")\n  // ...\n}","title":"How to make IntelliJ IDEA work with type safe BigQuery classes?"},{"location":"/FAQ.html#common-issues","text":"","title":"Common issues"},{"location":"/FAQ.html#what-does-mean-","text":"Sometimes you get an error message like Cannot prove that T1 <:< T2 when saving an SCollection. This is because some sink methods have an implicit argument like this which means element type T of SCollection[T] must be a sub-type of TableRow in order to save it to BigQuery. You have to map out elements to the required type before saving.\ndef saveAsBigQuery(tableSpec: String)(implicit ev: T <:< TableRow)\nIn the case of saveAsTypedBigQuery you might get an Cannot prove that T <:< com.spotify.scio.bigquery.types.BigQueryType.HasAnnotation. error message. This API requires an SCollection[T] where T is a case class annotated with @BigQueryType.toTable. For example:\n@BigQueryType.toTable\ncase class Result(user: String, score: Int)\n\np.map(kv => Result(kv._1, kv._2)).saveAsTypedBigQuery(args(\"output\"))\nNote Scio uses Macro Annotations and Macro Paradise plugin to implement annotations. You need to add Macro Paradise plugin to your scala compiler as described here.","title":"What does “Cannot prove that T1 <:< T2” mean?"},{"location":"/FAQ.html#how-do-i-fix-invalid-default-bigquery-credentials-","text":"If you don’t specify a secret credential file for BigQuery [1], Scio will use your default credentials (via GoogleCredential.getApplicationDefault), which:\nReturns the Application Default Credentials which are used to identify and authorize the whole application. The following are searched (in order) to find the Application Default Credentials: - Credentials file pointed to by the GOOGLE_APPLICATION_CREDENTIALS environment variable - Credentials provided by the Google Cloud SDK - gcloud auth application-default login command - Google App Engine built-in credentials - Google Cloud Shell built-in credentials - Google Compute Engine built-in credentials\nThe easiest way to configure it on your local machine is to use the gcloud auth application-default login command.\n[1] Keep in mind that you can specify your credential file via -Dbigquery.secret.","title":"How do I fix invalid default BigQuery credentials?"},{"location":"/FAQ.html#why-are-my-typed-bigquery-case-classes-not-up-to-date-","text":"Case classes generated by @BigQueryType.fromTable or other macros might not update after table schema change. To solve this problem, remove the cached BigQuery metadata by deleting the .bigquery directory in your project root. If you would rather avoid any issues resulting from caching and schema evolution entirely, you can disable caching by setting the system property bigquery.cache.enabled to false.","title":"Why are my typed BigQuery case classes not up to date?"},{"location":"/FAQ.html#how-do-i-fix-with-bigquery-","text":"BigQuery requests may sometimes timeout, i.e. for complex queries over many tables.\nexception during macro expansion:\n[error] java.net.SocketTimeoutException: Read timed out\nIt can be fixed by increasing the timeout settings (default 20s).\nsbt -Dbigquery.connect_timeout=30000 -Dbigquery.read_timeout=30000","title":"How do I fix “SocketTimeoutException” with BigQuery?"},{"location":"/FAQ.html#why-do-i-see-names-like-in-the-ui-","text":"Scio traverses JVM stack trace to figure out the proper name of each transform, i.e. flatMap@{UserAnalysis.scala:30} but may get confused if your jobs are under the com.spotify.scio package. Move them to a different package, e.g. com.spotify.analytics to fix the issue.","title":"Why do I see names like “main@{NativeMethodAccessorImpl...}” in the UI?"},{"location":"/FAQ.html#how-do-i-fix-error-","text":"You might see errors like RESOURCE_EXHAUSTED: IO error: No space left on disk in a job. They usually indicate that you have allocated insufficient local disk space to process your job. If you are running your job with default settings, your job is running on 3 workers, each with 250 GB of local disk space. Consider modifying the default settings to increase the number of workers available to your job (via --numWorkers), to increase the default disk size per worker (via --diskSizeGb).","title":"How do I fix “RESOURCE_EXHAUSTED” error?"},{"location":"/FAQ.html#can-i-use-trait-instead-of-method-","text":"Your Scio applications should define a main method instead of extending scala.App. Applications extending scala.App due to delayed initialization and closure cleaning may not work properly.","title":"Can I use “scala.App” trait instead of “main” method?"},{"location":"/FAQ.html#how-to-inspect-the-content-of-an-scollection-","text":"There is multiple options here: - Use debug() method on an SCollection to print its content as the data flows through the DAG during the execution (after the close or closeAndCollect) - Use a debugger and setup break points - make sure to break inside of your functions to stop control at the execution not the pipeline construction time - In Scio-REPL, use closeAndCollect() to close the context and materialize the content of an SCollection","title":"How to inspect the content of an SCollection?"},{"location":"/FAQ.html#how-do-i-improve-side-input-performance-","text":"By default Dataflow workers allocate 100MB (see DataflowWorkerHarnessOptions#getWorkerCacheMb) of memory for caching side inputs, and falls back to disk or network. Therefore jobs with large side inputs may be slow. To override this default, register DataflowWorkerHarnessOptions before parsing command line arguments and then pass --workerCacheMb=N when submitting the job.\nPipelineOptionsFactory.register(classOf[DataflowWorkerHarnessOptions])\nval (sc, args) = ContextAndArgs(cmdlineArgs)","title":"How do I improve side input performance?"},{"location":"/FAQ.html#how-do-i-control-concurrency-number-of-dofn-threads-in-dataflow-workers","text":"By default Google Cloud Dataflow will use as many threads (concurrent DoFns) per worker as appropriate (precise definition is an implementation detail), in same cases you might want to control this. Use NumberOfWorkerHarnessThreads option from DataflowPipelineDebugOptions. For example to use a single thread per worker on 8 vCPU machine, simply specify 8 vCPU worker machine type, and --numberOfWorkerHarnessThreads=1 in CLI or set corresponding option in DataflowPipelineDebugOptions.","title":"How do I control concurrency (number of DoFn threads) in Dataflow workers"},{"location":"/FAQ.html#how-to-manually-investigate-a-cloud-dataflow-worker","text":"First find the VM of the worker, the easiest place is through the GCE instance groups:\ngcloud compute ssh --project=<project> --zone=<zone> <VM>\nTo find the id of batch (for batch job) container:\ndocker ps | grep \"batch\\|streaming\" | awk '{print $1}'\nTo get into the harness container:\ndocker exec -it <container-id> /bin/bash\nTo install java jdk tools:\napt-get update\napt-get install default-jdk -y\nTo find java process:\njps\nTo get GC stats:\njstat -gcutil <pid> 1000 1000\nTo get stacktrace:\njstack <pid>\nFor maintainers, follow the following steps to update the table of contents.\nClone wiki repo git clone git@github.com:spotify/scio.wiki Create a new personal access token under https://github.com/settings/tokens Save new token in scripts/token.txt Run ./scripts/faq.sh Commit and push changes to FAQ.md","title":"How to manually investigate a Cloud Dataflow worker"},{"location":"/Powered-By.html","text":"","title":"Powered By"},{"location":"/Powered-By.html#powered-by","text":"Here is a list of organizations using Scio in production.\nOrganization Use Case Code Spotify Everything including music recommendation, monetization, artist insights and business analysis. We also use BigQuery, Bigtable and Datastore heavily with Scio. Big Data Rosetta Code, Ratatool, Featran Big Viking Games Streaming event collection and ETL using Pub/Sub and BigQuery Algolia Log collection and analytics using Bigtable, Cloud Storage & Pub/sub Hypefactors Natural language processing / media monitoring. Also using PubSub, GCS and ElasticSearch with Scio. Discord Streaming event collection, sessionization, and enrichment using Pub/Sub, BigQuery, and Bigtable. Dow Jones Streaming article events, bulk article extractions and ETL using Pub/Sub, GCS and BigQuery for the DNA Platform. Honey Streaming ETL data pipeline from Pub/Sub to Pub/Sub, BigTable, BigQuery. Cabify Streaming data pipelines from Pub/Sub to BigQuery Jobrapido Streaming and Batch ETL using Pub/Sub and BigQuery 9GAG Streaming and Batch ETL using Pub/Sub Cityblock Streaming and Batch ETL using Pub/Sub, BigQuery, and Datastore Arquivei Streaming and Batch ETL using Pub/Sub, BigQuery, GCS, S3, and ElasticSearch Vpon Batch ETL and BigQuery Snowplow Analytics Streaming ETL Beam Enrich, BigQuery Loader and Cloud Storage Loader","title":"Powered By"},{"location":"/extras/Algebird.html","text":"","title":"Algebird"},{"location":"/extras/Algebird.html#algebird","text":"Algebird is Twitter’s abstract algebra library. It has a lot of reusable modules for parallel aggregation and approximation. One can use any Algebird Aggregator or Semigroup with: - aggregate and sum on SCollection[T] - aggregateByKey and sumByKey on SCollection[(K, V)]\nSee AlgebirdSpec.scala and Algebird wiki for more details. Also see these slides on semigroups.","title":"Algebird"},{"location":"/extras/Algebird.html#algebird-in-repl","text":"scio> import com.twitter.algebird._\nscio> import com.twitter.algebird.CMSHasherImplicits._\nscio> val words = sc.textFile(\"README.md\").\n     | flatMap(_.split(\"[^a-zA-Z0-9]+\")).\n     | filter(_.nonEmpty).\n     | aggregate(CMS.aggregator[String](0.001, 1E-10, 1)).\n     | materialize\nscio> sc.close()\nscio> val cms = words.waitForResult().value.next\nscio> cms.frequency(\"scio\").estimate\nres2: Long = 19\n\nscio> // let's validate:\nscio> import sys.process._\nscio> \"grep -o scio README.md\"  #| \"wc -l\"!\n      19","title":"Algebird in REPL"}]}